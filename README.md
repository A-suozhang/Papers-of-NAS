# Awesome-Neural-Architecture-Search

> ä¾¿äºä¸ªäººæŸ¥æ‰¾çš„ä¸€äº›NASæ–‡ç« çš„æ¢³ç†ä»¥åŠç®€è¦Digest
> é‡‡ç”¨äº†ä¸[Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)ä¸åŒçš„é€æ¨¡å—   æ¢³ç†æ–¹å¼ï¼Œä¾¿äºä¸ªäººç†è§£ä¸é€ŸæŸ¥

## Genre


## Paper Digest


#### [Overcoming Multi-Model Forgetting in One-Shot NAS with DiversityMaximization](https://shiruipan.github.io/publication/cvpr-2020-zhang/)
* ğŸ”‘ Key:
  * è§£å†³ä¼ ç»Ÿçš„Shared-Weightsæ–¹æ³•åœ¨ä¼˜åŒ–æ–°çš„æ¶æ„çš„æ—¶å€™è€çš„æ¶æ„ç²¾åº¦ä¼šä¸‹é™(Catastrophic Forgetting)çš„é—®é¢˜
* ğŸ“ Sourceï¼š
  * CVPR 2020
* ğŸŒ± Motivation: 
    * ä¼ ç»Ÿçš„OneShotæ–¹å¼è®¤ä¸ºJointly Optimized Supernet Weightsæ˜¯æœ€ä¼˜çš„
    * ä½†æ˜¯sequentially train archs with partially-shared weightsä¼šå¯¼è‡´Catastrophic Forgetting
    * æ–‡ç« æ ¸å¿ƒæŠŠOne-ShotNASçœ‹æˆä¸€ä¸ªContinual Learningçš„é—®é¢˜(Constrained Optimzation,learning of current arch should not degrade previous much)
* ğŸ’Š Methodology:
    * NSAS(Search-based Architecture Selection) Loss Function 
    * Enforce the architectures inheriting weights from the supernet in current step perform better than last step
    * å¦‚æœç´¯è®¡çš„è¯è¦æ±‚ä¼šå¤ªé«˜äº†ï¼Œæ‰€ä»¥ä¸æ˜¯é™å®šå…¨éƒ¨çš„Previous Archï¼Œè€Œæ˜¯é€‰æ‹©å…¶ä¸­çš„ä¸€ä¸ªSubset(å¯¹äºå¦‚ä½•é€‰å®šè¿™ä¸ªSubsetæ˜¯å‡å®šSubsetä¸­çš„Archè¦æœ‰Diversity-æ‰¾åˆ°æœ€å¤§Diversityçš„è¿‡ç¨‹å°±æ˜¯æ‰€ä¸ºçš„Novelty Search)
    * å®ç°çº¦æŸçš„æ–¹å¼æ˜¯åŠ ä¸€ä¸ªSoft Regularization

#### [One-Shot Neural Architecture Search via Self-Evaluated Template Network](https://arxiv.org/abs/1910.05733)
* ğŸ”‘ Key:
  * ä¼ ç»Ÿçš„Evluationæ…¢ï¼ŒShared Weightsçš„æ–¹å¼é€‰å–å»Evaluateçš„ç»„ä»¶çš„æ—¶å€™æ˜¯Randomçš„ï¼Œä¸å¤ŸInstructive
* ğŸ“ Sourceï¼š
  * ICCV 2019
* ğŸŒ± Motivation: 
  * æå‡ºäº†ä¸€ä¸ªSETN(Self Evaluated Template Network)
    * ä¸€ä¸ªEvaluatorå»é¢„æµ‹æœ‰æ›´ä½Valid Lossçš„æ¶æ„(ç±»ä¼¼ä¸€ä¸ªPredictor)
    * ä¸€ä¸ªæ¨¡æ¿Templateç½‘ç»œå»Shared Params,åŒ…å«äº†æ‰€æœ‰çš„Candidate
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402210522.png)
    * çœ‹ä¸Šå»å°±æ˜¯å¯¹Shared-weightsåŠ äº†ä¸€ä¸ªPredictorä½œä¸ºControllerï¼Œå»ä»ä¸€ä¸ªæ‰€è°“çš„Templateç½‘ç»œä¸­é‡‡æ ·å‡ºå­æ¶æ„ï¼ŒControlleræ¥å†³å®šæ€ä¹ˆé‡‡(è€Œä¸æ˜¯éšæœºé‡‡æ ·)
  * Nä¸ªCell
    * æ¯ä¸ªCellä¸­Bä¸ªBlock
    * æ¯ä¸ªBlockå¯èƒ½æ˜¯4å…ƒç»„
  * Candidate Network 
    * Contain All candidate CNN in search space
    * train stochsticly - uniformly sample 1 candidate and only optimize its params
      * Optimize each with equal possibility
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402212418.png)
      * Iæ˜¯inputæ˜¯çº¯éšæœºSampleï¼ŒFæ˜¯Functionï¼Œå…¶ä¸­çš„orderæŒ‡çš„æ˜¯å†ä¸€ä¸ªé›†åˆOä¸­é‡‡æ ·ï¼Œå…¶ä¸­f1çš„indexä¸€å®šè¦å°äºf2
    * Evaluatorï¼š
      * Encode one CNN candidate as a set of quadruples
      * ä»categorical distribution sampleå‡ºä¸€ä¸ªchoiceï¼Œç”¨softmax normalized valueä½œä¸ºvectorå€¼
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402213049.png)
* ğŸ’¡ Ideas:
    * æŒ‡å‡ºæœ‰äº›Shared-Weightsçš„æ–¹æ³•ä¼šshared parameter with a learnable distribution of archs
      * ä½œè€…è®¤ä¸ºè¿™æ ·ä¼šæœ‰biasï¼Œå› ä¸ºç›¸å¯¹lightweightçš„modelä¼šæ›´å¿«æ”¶æ•›ï¼Œlearnable distribution will bias to these model
      * â€œthe Matthew effectâ€ to refer that some quickly-converged candidates will get more chances  to be further optimized in some NAS algorithms


#### [GATES]()
* ğŸ”‘ Key: 
  * NASä¸­çš„**Predictor**é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªæ›´å¥½çš„Encoder
* ğŸ“ Source
  * Arxiv & THU EE
* ğŸŒ± Motivation: 
  * Current Encoder model topological information implicitly
  * åŸæœ¬çš„gcnä¹‹ç±»çš„encoderæ–¹å¼edges stand for notion of affinity, feature on node
  * View NN as Data-Processing Graph
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200404200848.png)
  * è®¤ä¸ºä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿç›´æ¥Handle Topological isomorphism,ä»¥åŠå¯¹Information Processingçš„å»ºæ¨¡
* ğŸ“ Exps: 
  * Nb101/201ä¸ŠKDï¼ŒNatKï¼ŒPatK
* ğŸ’¡ Ideas:
  * ç›¸å…³æ–‡ç« NAOï¼Œç”¨ä¸€ä¸ªEncoder-Decoderæ¥åš
  * In the Kendall's Tau measure, all discordant pairs are treated equally
  * å…¶ä»–æŒ‡æ ‡(å› ä¸ºkendall tauå…¶å®è€ƒè™‘äº†å¾ˆå¤špoor archçš„ç›¸å¯¹å…³ç³»ï¼Œå¯¹äºNASæ¥è¯´ç”¨å¤„ä¸å¤§)
    * NatKï¼š predictç»™å‡ºçš„Kä¸ªä¸­æœ€å¥½çš„å®é™…rank
    * PatKï¼š Predictor TopKåœ¨Gt TopKä¸­çš„æ¯”ä¾‹
  * Nb101 - 432k archs - Op on Node
  * Nb201 - 15625 - OP on Edge














## Reference

* [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)

 
