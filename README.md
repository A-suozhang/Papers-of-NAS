# Awesome-Neural-Architecture-Search

> ‰æø‰∫é‰∏™‰∫∫Êü•ÊâæÁöÑ‰∏Ä‰∫õNASÊñáÁ´†ÁöÑÊ¢≥ÁêÜ‰ª•ÂèäÁÆÄË¶ÅDigest
> ÈááÁî®‰∫Ü‰∏é[Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)‰∏çÂêåÁöÑÈÄêÊ®°Âùó   Ê¢≥ÁêÜÊñπÂºèÔºå‰æø‰∫é‰∏™‰∫∫ÁêÜËß£‰∏éÈÄüÊü•

## Paper List

> ÊúâÁºñÂè∑ÁöÑÊñáÁ´†ÊòØÊó©ÊúüÊØîËæÉÈáçË¶ÅÁöÑÊñáÁ´†ÔºåFollowÂÖ∂ÁöÑÊñáÁ´†Ôºå‰ºöÁî®9-AÁöÑÊ†ºÂºèÊù•Ê†áÊ≥®

|Title üìï|Source üéì|Code üíª|Component üî®|Property üí†|
|--|--|--|--|--|
|[1. Neural Architecture Search with Reinforcement Learning](https://arxiv.org/pdf/1611.01578)|ICLR2017(1611) *Zoph.* at Google Brain| - | Flow |NAS Flow|
|[2. Accelerating Neural Architecture Search Using Performance Prediction](https://arxiv.org/pdf/1705.10823)|ICLR2018W(1705) *Baker* at MIT| - |Evaluator|Predictor-based Evaluator|
|[3. eNAS - Efficient Architecture Search by network transformation](https://arxiv.org/abs/1707.04873)|AAAI2018(1707) *Cai* at SJTU|-|Flow/Weights-Manager|Shared Weights/Mutation from Existing Network/RL Controller|
|[4. Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012.pdf)|CVPR2018(1707) *Zoph* Google Brain|-|Search Space|Cell-based Search Space|
|[5. HyperNet - SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)|ICLR2017 *Brook*|-|Weights-Manager/Evaluator|HyperNet 2 Produce SubNet's Weight|
|[5-A. Graph HyperNetwork for Neural Architecture Search](https://arxiv.org/abs/1810.05749)|ICLR2019 *Chris Zhang* Toronto|-|Weights-Manager/Evaluator|HyperNet 2 Produce SubNet's Weight|
|[6. ENAS - Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)|ICML2018(1802) *Pham* (at) Google Brain|-|Flow/Weight-Manager/Evaluator|Shared Weights Flow|
|[7. DARTS - Differentiable Architecture Search](https://arxiv.org/pdf/1806.09055)|ICLR2019(1806) *Liu* (at) Google Brain|-|Flow/Controller|Gradient-based Flow|
|[7-A. SNAS - Stochastic Architecture Search](https://arxiv.org/pdf/1812.09926)|ICLR2019(1812) *Xie* (at) SenseTime |-|Controller|Gradient-based Flow|
|[7-B. DARTS-nds - On Network Design Spaces for Visual Recognition](https://arxiv.org/pdf/1905.13214.pdf)|ICCV2019(1905) *IIija* (at) FAIR |-|SS|Improved NAS SS|
|[8. Hierarchical Representations for Efficient Architecture Search](https://arxiv.org/pdf/1711.00436)|ICLR2018(1711) *Liu* (at) Google Brain|-|Search Space|Hierarchical SS|
|[9. Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)|ECCV2018(1712) *Liu* (at) Google AI|-|Controller|Predictor-based/Easy2Hard|
|[10. NAO - Neural Architecture Optimization](https://arxiv.org/abs/1808.07233)|NIPS2018(1808) *Luo* (at) MSRA|-|Evaluator|Predictor-based/Gradient-based|
|---------------------------------------------|-----------------|-----|--------------------|--------------------|
|[DetNAS: Backbone Search for Object Detection](http://arxiv.org/abs/1903.10979)|Arxiv(1903) *SunJian* at Megvii|-|Task|Shared-Weights4DetBackbone|
|[NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](http://arxiv.org/abs/1904.07392)|Arxiv(1904) *Quo V Le* at Google Brain|-|Task/Search Space|Search for FPN|
|[EfficientDet: Scalable and Efficient Object Detection](http://arxiv.org/abs/1911.09070)|Arxiv(1911) *Quo V Le* at Google Brain|-|Task/Search Space|BiFPN+Weighted+Scalable Arch|
|---------------------------------------------|-----------------|-----|--------------------|--------------------|
|[A Survey on Neural Architecture Search](https://arxiv.org/pdf/1905.01392.pdf)|Arxiv(1905) *Martin* at IBM|-|Survey|-|
|[Accelerator-Aware Neural Network Design Using AutoML](https://arxiv.org/abs/2003.02838)|MLsys20-W Gupta|-|Hardware|NAS4Accelerator|
|[MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning](https://arxiv.org/abs/2003.14058)|CVPR2020 Gao|-|Flow|NAS + MultiTasking|
|[GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet](https://arxiv.org/abs/2003.11236)|CVPR2020 You|-|Weights-Manager/Evaluator|Improvement of HyperNet|
|[How to Train Your Super-Net: An Analysis of Training Heuristics in Weight-Sharing NAS](https://arxiv.org/abs/2003.04276)|Arxiv(2003)|-|One-Shot|Analysis for Training One-Shot|
|[Disturbance-immune Weight Sharing for Neural Architecture Search](https://arxiv.org/abs/2003.13089)|Arxiv(2003) Niu|-|One-Shot|Improve Weight -Sharing|
|[NPENAS:Neural Predictor Guided Evolution for Neural Architecture Search](https://arxiv.org/abs/2003.12857)|Arxiv(2003) Wei|-|Predictor|Predictor+Evo|
|[DA-NAS:Data Adapted Pruning for Efficient Neural Architecture Search ](https://arxiv.org/abs/2003.12563)|Arxiv(2003) Dai|-|Flow|Speed up NAS flow via triming blocks|
|[MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation](https://arxiv.org/abs/2003.12238)|Arxiv(2003) He|-|Gradient-based Flow|Bi-level Optimization lead to Sub-optimal|
|[Are Labels Necessary for Neural Architecture Search? ](https://arxiv.org/abs/2003.12056)|Arxiv(2003) Liu & Kaiming|-|Evaluator|Unsupervised Learning for NAS Evaluator|
|[Sampled Training and Node Inheritance for Fast Evolutionary Neural Architecture Search](https://arxiv.org/abs/2003.11613)|Arxiv(2003) Zhang|-|Controller|Faster EVO|
|[BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models](https://arxiv.org/abs/2003.11142)|Arxiv(2003) Yu|-|Shared-Weights|No Retraining Weight(like OFA)|
|[PONAS: Progressive One-shot Neural Architecture Search for Very Efficient Deployment](https://arxiv.org/abs/2003.05112)|Arxiv(2003) Huang|-|Evaluator|Progressive+One-shot|
|[Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting](https://arxiv.org/abs/2003.10392)|Arxiv(2003) Wu|-|Evaluator|Improved NAO Flow,escape Local minima|
|[Real-time Federated Evolutionary Neural Architecture Search](https://arxiv.org/abs/2003.02793)|Arxiv(2020) Zhu|-|Flow|Evo+Federated Learning|
|[ADWPNAS: Architecture-Driven Weight Prediction for Neural Architecture Search](https://arxiv.org/abs/2003.01335)|Arxiv(2020) Zhang|-|Evaluator(HyperNet)|HyperNet Improve|
|[BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of Channels](https://arxiv.org/abs/2003.09821)|Arxiv(2003) Shen|-|Evaluator/Weights-Manager|Controllable Shared-Weights|
|[Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection](https://arxiv.org/abs/2003.11818)|Arxiv(2003) Guo|-|Task|HierNAS + Det|
|[DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation](https://arxiv.org/abs/2003.11883)|Arxiv(2003) Zhang|-|Task|NAS4Semantic Seg|
|[Probabilistic Dual Network Architecture Search on Graphs](https://arxiv.org/abs/2003.09676)|Arxiv(2003) Zhao|-|Task|NAS4GNN|
|[GAN Compression: Efficient Architectures for Interactive Conditional GAN](https://arxiv.org/abs/2003.08936)|Arxiv(2003) Li|-|Task|NAS4GAN|
|[ElixirNet: Relation-aware Network Architecture Adaptation for Medical Lesion Detection](https://arxiv.org/abs/2003.08770)|Arxiv(2003) Jiang|-|Task|NAS+Medical|
|[Lifelong Learning with Searchable Extension Units](https://arxiv.org/abs/2003.08559)|Arxiv(2003) Wang|-|Task|NAS+Continual Learning|
|[Hierarchical Neural Architecture Search for Single Image Super-Resolution](https://arxiv.org/abs/2003.04619)|Arxiv(2003) Guo|-|Task|HierNAS4SR|
|[BATS: Binary ArchitecTure Search](https://arxiv.org/abs/2003.01711)|Arxiv(2020) Bulat|-|Task|NAS4BNN|
|[NAS-Count: Counting-by-Density with Neural Architecture Search](https://arxiv.org/abs/2003.00217)|Arxiv(2020) Hu|-|Task|Counting|








## Paper Digest


```
---------------Format---------------
* üîë Key:         Ê†∏ÂøÉ
* üéì Source:      Êù•Ê∫ê
* üå± Motivation:  ÊïÖ‰∫ã
* üíä Methodology: ÊñπÊ≥ï
* üìê Exps:        ÂÆûÈ™å
* üí° Ideas:       ÊÉ≥Ê≥ï
-----------------------------------
```


#### 1. [NAS with RL](https://arxiv.org/pdf/1611.01578.pdf) - Google Brain
* üîë Key:
  * NAS Work Flow
* üéì SourceÔºöICLR2017 / Google Brain
* üå± Motivation:
  * È¢ÜÂüüÂºÄÁ´ØÔºåNASÁöÑÂºÄÂ±±,Â•†ÂÆö‰∫ÜÂ§ßËá¥Workflow
  * Âª∫Á´ãÂú®ËÆ§‰∏∫NNÁöÑconnectivityÂíåstructureÂèØ‰ª•Ë¢´Ë°®ËææÊàê‰∏Ä‰∏™variable-length stringÔºàÂèØÁî®RNNÂÅöembeddingÔºå‰πüÂ∞±ÊòØ‰Ωú‰∏∫controllerÔºâ
    * Â∞ÜÈááÊ†∑ÔºàRNNÁîüÊàêÔºâÂá∫Êù•ÁöÑÂ≠êÊû∂ÊûÑËøõË°åËÆ≠ÁªÉÔºåÂ∞ÜAcc‰Ωú‰∏∫reward‰ø°Âè∑ÔºåÈÄöËøáPolicy GradientÁöÑÊñπÂºèÂõû‰º†Âéªupdate controllerÔºàOriginal RNNÔºâ
* üíä Methodology: 
  * search space
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203201724.png)
  * RNN
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203202126.png)
    * Â∞ÜRNNÁúã‰Ωú‰∏Ä‰∏™Ê†ëÁöÑÁªìÊûÑ
* üìê Exps: 
  * ÊØîcifar10‰∏äÁöÑbaselineÊèê‰∫Ü0.1‰∏™ÁÇπÔºåÂπ∂‰∏îÂø´‰∫Ü‰∏Ä‰∏¢‰∏¢1.05Ôºå‰ª•ÂèäÂÖ∂‰ªñÁöÑdataset-PennTreebank‰ª•Âèälanguage modelingÁöÑtask
* üí° Ideas:
  * Âú®relatedwork‰∏≠ËÆ≤Âà∞
    * hyper-param optimizationÂè™ËÉΩÂ§üÂÅöÂà∞Âú®fixed-lengthÁöÑspaceËøõË°åÊ®°Âûã‰ºòÂåñÔºå‰∏îÂØπgood initial modelÊØîËæÉ‰æùËµñ
    * BayesianÊñπÊ≥ïÂèØ‰ª•ÂØªÊâæ‰∏Ä‰∏™‚Äú‰∏çÂÆöÈïø‚ÄùÁöÑÊû∂ÊûÑÔºå‰ΩÜÊòØ‰∏çÊòØÂæàÊúâÊÑè‰πâ
  * The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning. 


#### 2. [Accelerating NAS using performance prediction](https://arxiv.org/pdf/1705.10823.pdf)

* üîë Key: 
  * **Predictor**        
* üéì Source: ICLR2018W / CMU   
* üå± Motivation:  
  * **ÊèêÂá∫predictorÊù•Âä†ÈÄüevaluation**,‰ª•ÂèäEarlyStopping
  * ËÆ§‰∏∫humanÊòØ‰ªétraining curveÊù•ËßÇÂØüÁöÑÔºåÊú¨Êñáparameterize‰∫ÜËøô‰∏™ËøáÁ®ãÔºåËÆ≠ÁªÉregression modelÊù•È¢ÑÊµãacc
  * ÂØπÂ∫îÁöÑcounterpartÊòØBayesianÁöÑ‰∏Ä‰∫õÊñπÊ≥ï
    * ÁõÆÊµãÊòØ‰∫∫Â∑•ËÆæËÆ°‰∏Ä‰∫õÊãüÂêàlearning curveÁöÑbase functionÔºåÁÑ∂ÂêéÁî®expensiveÁöÑMCMCÊù•ÊãüÂêà
    * ‰πüÊúâÁî®È´òÊñØËøáÁ®ãÊ†∏ÂáΩÊï∞ÁöÑ
* üíä Methodology: 
  * ËØ¥ÁôΩ‰∫ÜÂª∫Ê®°ÁöÑÊòØTraining Curve
    * ËæìÂá∫val accÔºåËæìÂÖ•ÊòØconfigurationÔºåÂú®ÊØè‰∏™time step
    * ÂÅö‰∫Ü‰∏Ä‰∏™SRMÔºàSequential Regression modelÔºâÊØîÂ¶ÇRBMÔºåRandomForestÁ≠â
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203204919.png)
* üìê Exps:        
* üí° Ideas:      



#### 3. [eNAS - Efficient Architecture Search by Network Transformation](https://arxiv.org/pdf/1707.04873.pdf)

* üîë Key:  
  * **Weight Manager** -> Shared Weights
* üéì Source:  AAAI2018 / SJTU
* üå± Motivation:  
  * **Reusing Weight(Weight Manager)‰∏çÈúÄË¶Åfrom scratchÊù•ËÆ≠ÁªÉÁΩëÁªú**
  * metacontrollerÈááÊ†∑Êû∂ÊûÑbased on Network transformation 
* üíä Methodology: 
    * ‰æùÁÑ∂ÊòØRl Controller - Policy GradientÊõ¥Êñ∞ÔºåÂÖ∂actionÊòØÊåáÂØºtransformationÔºåÁΩëÁªúÁöÑÁªìÊûÑencoder‰æùÁÑ∂ÊòØ‰∏Ä‰∏™Bi-LSTM
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190254.png)
      * Êúâ‰∏§ÁßçActorÂΩ¢ÂºèÔºåNet2WiderÊàñËÄÖNet2Deeper
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190452.png)
      * WiderÁî®‰∏Ä‰∏™ÂÖ¨Áî®ÁöÑsigmoidÂàÜÁ±ªÂô®Êù•ÁîüÊàêÂèçÈ¶à‰ø°ÊÅØÔºåÁ°ÆÂÆöÊòØÂê¶Âú®ËØ•Êó∂ÂàªÈúÄË¶Åexpand
      * Net2DeeperÂàôÊòØÊòØÂê¶ÈúÄË¶Åinsert‰∏Ä‰∏™Êñ∞ÁöÑlayer
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190710.png)
      * DeeperÁî®‰∏Ä‰∏™RNNÊù•Âª∫Ê®°ÊòØÂê¶ÈúÄË¶ÅÂú®Êüê‰∏™‰ΩçÁΩÆÊèíÂÖ•‰∏Ä‰∏™Êñ∞Â±Ç
      * ÂèØ‰ª•ÊèíÂÖ•Êñ∞Â±ÇÁöÑ‰ΩçÁΩÆÊòØÈ¢ÑÂÖàÂõ∫ÂÆöÁöÑÔºå‰æùÊçÆpoolingÂ±ÇÁöÑ‰ΩçÁΩÆÊääÁΩëÁªúÂàÜÊàêÂá†‰∏™block
* üìê Exps:       
* üí° Ideas:   
    * (‰πüÂèØ‰ª•ËÆ§‰∏∫ÊòØmuation-basedÔºå‰∏çËøáÊåáÂØºmutation‰∫ßÁîüÁöÑ‰∏çÊòØÈÅó‰º†ÊàñËÄÖSAËøôÁ±ªheuristicÁöÑÊñπÊ≥ïÔºå‰ΩÜÊòØÊòØRL-Agent)


#### 4. [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012.pdf)
* üîë Key:      
  * **Cell-based Search Space** (NASNet SS)   
* üéì Source:     
  * CVPR2018 / Google Brain 
* üå± Motivation:  
  * Â∞ÜSearchSpaceÂàÜÊàêÂ§ö‰∏™Áõ∏ÂêåCellÁöÑÂ†ÜÂè†
* üíä Methodology:
  * Convolutional Blocks Repeated many times
    	* ‰∏ÄËà¨ÂàÜÊàêN‰∏™Normal CellhÂêéÈù¢Êé•‰∏Ä‰∏™Reduction Cell
  * Âü∫‰∫éRNNÁöÑController
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185419.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185525.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185626.png) 
* üìê Exps:       
* üí° Ideas:    
	* Regularization Technique - Schedule Drop Path



#### 5. [SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)

* üîë Key:  
  * **HyperNet** -> Produce Weight
  * **One-shot NAS Framework** -> Shared Weights      
* üéì Source:  
  * ICLR2017 
* üå± Motivation: 
  * **one-shotË°®Á§∫‰ªé‰∏Ä‰∏™HyperNet‰∏≠ÂèñÂá∫Êû∂ÊûÑÔºå‰∏çÈúÄË¶Åtraining‰ΩúevaluationÔºåÈááÁî®weight sharing** 
  * HyperNetÁî®Êù•ÂØπÁªôÂÆöÁöÑÊû∂ÊûÑÁîüÊàêWeightsÔºåÂÅöÂà∞Fast Eavluation
* üíä Methodology: 
   * HyperNet - Training an auxiliary HyperNet to generate weights
     * Âä†ÈÄüarch selection
     * ‰ªébinary codedÂà∞optimal architecture weightsÁöÑmappingÔºåÂè™ËÆ≠ÁªÉoutput layer(?)
     * ÂÖ∂ËÆ≠ÁªÉÊòØÁî®gradient-basedÁöÑÊñπÂºèÊù•ÂÅöÂà∞ÁöÑ
* üìê Exps:       
* üí° Ideas:      
   * ÈÄâÂèñarchÊù•evluateÁöÑÊñπÂºè
     * Memory-bank viewÔºåÂ∞ÜÂÖ∂‰Ωú‰∏∫binary vector
     * ÊòØÂê¶ÊÑèÂë≥ÁùÄ‰ºöÈÅçÂéÜÊï¥‰∏™search spaceÔºü
   * ÊñáÁ´†‰∏≠ÊèêÂà∞‰∫ÜËØ¥*ËÆ≠ÁªÉ‰∏Ä‰∏™archÂºÄÂßãÁöÑÈÉ®ÂàÜÊòØÊï¥‰ΩìaccÁöÑ‰∏Ä‰∏™insight*
     * ‰∏ÄËà¨ÁöÑÊñπÊ≥ïÊääarchÁöÑperfÁúã‰Ωú‰∏Ä‰∏™black boxÔºåÁî®BOÊàñËÄÖrsÂéªÊêúÁ¥¢Ôºå‰πüÊúâearly-stoppingËøôÊ†∑ÁöÑÁ≠ñÁï• 
   * ËÉΩÂ§üÊäõÂºÉÂÖ∂‰ªñÊâÄÊúâÁöÑhyper-param‰ª•Âèädynamic-regulariaztionÁöÑ‰∏úË•ø
     * ÊØîÂ¶Çlr schedule
     * ÊØîÂ¶ÇDropPath‰πãÁ±ªÁöÑ‰∏úË•ø
   * [Meta-Pruning(ICCV2019)](https://arxiv.org/pdf/1903.10258)ÂíåËøô‰∏™ÊúâÁÇπÂÉèÁöÑ
   * We hypothesize that so long as the HyperNet learns to generate reasonable weights, the validation
error of networks with generated weights will correlate with the performance when using normally
trained weights



#### 5-A. [Graph HyperNetwork for Neural Architecture Search](https://arxiv.org/pdf/1810.05749.pdf)
* üîë Key:  
  * ÂõæÂΩ¢ÂºèÁöÑHypernet
* üéì Source:  
  * ICLR 2019 && Uber & Toronto Univ.
* üå± Motivation: 
  * 1st to Generate All Weights
  * Â§öÁßçHyperNetÁöÑÊñπÂºèÔºö
    * 3D Encoding Tensor
    * LSTM Process a Sequence
* üíä Methodology: 
  * SS
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413183600.png)
  * GNN(ÁîüÊàêGraph Embedding)ÊØè‰∏™NodeÊòØ‰∏Ä‰∏™LSTMÔºåÁîüÊàê‰∏Ä‰∏™Embedding
  * HyperNetÂ∞±ÊòØ‰∏Ä‰∏™MLPÔºåÂØπÊâÄÊúâNodeÂÖ¨Áî®ÔºåËæìÂá∫Â§ßÂ∞èÂõ∫ÂÆö
    * ÂØπ‰∫é‰∏çÂêåÂ§ßÂ∞èÁöÑWeightÔºåÁî®Â†ÜÂè†KernelÊàñËÄÖÊòØChannelËææÊàê
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413183534.png)
  * *AnyTime Prediction* *Forward Backward Pass*  
    * Êõ¥Êñ∞ÁöÑÊ≠•È™§‰ªøÁÖßÁΩëÁªúÂâçÂêë‰ª•ÂèäÂèçÂêëÁöÑÊ≠•È™§
    * TimeStep 2V-1
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413182757.png)
* üí° Ideas:     
  * ÊâÄË∞ìMotifÁöÑÊñπÂºèÊõ¥‰∏∫È´òÊïàÔºåÊêúÁ¥¢‰∏Ä‰∏™CellÔºåÂØπ‰∫éÊñ∞ÁöÑ‰ªªÂä°ÔºåÂ∞±ÊîπÂèòCellÁöÑÂ†ÜÂè†Ê®°ÂºèÂ∞±ÂèØ‰ª•‰∫Ü

#### 6. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)

* üîë Key:
  * **SuperNet**
  * **One-Shot** -> Shared WeightsÔºàCurrent Popular FlowÔºâ     
* üéì Source:      
  * ICML 2018 / Google Brain
* üå± Motivation:
  * ÊûÑÂª∫‰∏Ä‰∏™SupernetÔºåËÆ§‰∏∫ÊâÄÊúâÊû∂ÊûÑÈÉΩÊòØËøô‰∏™SuperNetÁöÑComputation GraphÁöÑ‰∏Ä‰∏™SubGraphÔºåÂêÑ‰∏™Child Module Share Params  
* üíä Methodology:
   * ControllerÂú®‰∏Ä‰∏™Â∫ûÂ§ßÁöÑCompuation Graph‰∏äÊêúÁ¥¢Subgraph‰Ωú‰∏∫Â≠êÂõæ
    * ÂØπ‰∫échild modules Share Parameter  
  * RNN Controller
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308232519.png)
      * ÈááÊ†∑Âá∫ÈìæÊé•ÂÖ≥Á≥ª
      * ‰ª•ÂèänodeÊòØ‰ªÄ‰πà
    * Ëøô‰∏™controllerÊòØÊÄé‰πàËÆ≠ÁªÉÁöÑÔºü
      * Policy GradientÔºü
* üìê Exps:       
* üí° Ideas:  



#### 7. [DARTSÔºöDifferentiable Architecture Search](https://arxiv.org/pdf/1806.09055.pdf)

* üîë Key:     
  * **Á¶ªÊï£ÊêúÁ¥¢ -> Gradient-based**    
* üéì Source:   
  * ICLR 2019 / Google Brain   
* üå± Motivation: 
  * Â∞ÜÂéüÂÖàÁöÑÁ¶ªÊï£ÊêúÁ¥¢ÔºåÊîπ‰∏∫DifferentiableÊñπÂºè(Relax the Search space to be Differentiable),‰ª•ÊèêÈ´òÊïàÁéáÔºÅ
  * Ê†∏ÂøÉÂú®‰∫éÂ¶Ç‰Ωï‰øÆÊîπSearch Space 
* üíä Methodology: 
  * Search SpaceÂ¶Ç‰ΩïËÆæËÆ°
    * ‰∏Ä‰∏™CellÊòØ‰∏Ä‰∏™DAGÔºåNodeÊòØ‰∏Ä‰∏™latent representation(‰ª£Ë°®Feature Map)ÔºåÊØè‰∏™ÊúâÂêëÁöÑËæπÂíåÊüêÁßçÊìç‰Ωú(Op)ÊúâÂÖ≥
      * ÊØè‰∏™op‰ª•softmaxÊù•relaxÔºåÂèñÂêÑ‰∏™ÂÆûÈôÖÊìç‰Ωú(Max-pool/Conv/No)ÂèëÁîüÁöÑÊ¶ÇÁéá
    * ËÆ§‰∏∫ÊØè‰∏™cellÊúâ‰∏§‰∏™ËæìÂÖ•‰∏Ä‰∏™ËæìÂá∫ÔºåÂØπÂç∑ÁßØÂ±ÇËæìÂÖ•Êù•Ëá™previous 2 layer(? 2Â∫¶ËøëÈÇªÂ±ÖÔºü)
    * N=7 Node,Ê≤°Êúâstrided
  * Bi-level Optimization
    * Learn Arch and Weight at the same time
    * Ê≥®ÊÑèÂÆûÈôÖÂØπalphaÂØºÊï∞ÊòØÁî®ÁöÑÔºåËÄå‰∏çÊòØÂçïÁ∫ØÁöÑ \partial{L_val}/\partrial{\alpha}
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312222701.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312221224.png)
* üìê Exps:       
* üí° Ideas: 



#### 8. [Hierarchical Representations for Efficient Architecture Search](https://shimo.im/sheets/TkdXd9ptKTjDY83R/MODOC)
* üîë Key:     
  * **Hierarchical Search Space** 
* üéì Source:   
  * ICLR2018 / Google Brain   
* üå± Motivation:  
  * ÊµÅË°åÁöÑCell-BasedÁöÑÊû∂ÊûÑÁõ∏ÂØπgeneralÔºå‰ΩÜÊòØÊúâpredefinedÁöÑmeta-arch(ÊØîÂ¶ÇËøôÂá†‰∏™CellÂ∫îËØ•ÊÄé‰πàÂ†ÜÂè†‰πãÁ±ªÁöÑ)‰∏çÂ§ügeneral
* üíä Methodology: 
  * hierarchical genetic representation
    * Ê®°‰ªøÁöÑÊòØmodularized design pattern
  * ÈááÁî®‰∫ÜEAÔºåÊåáÂá∫ÊúÄnaiveÁöÑrandom search‰πüÂèØ‰ª•Ëé∑Âæó‰∏çÈîôÁöÑÊïàÊûú
  * flat representation - Â∞ÜNN‰Ωú‰∏∫‰∏Ä‰∏™DAG
    * Â∞èÁöÑgraph motifÁªÑÊàê‰∏Ä‰∏™Â§ßÁöÑgraph motif
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308204233.png)
  * Tournament Selection Search
* üìê Exps:       
* üí° Ideas: 

#### 9. [PNAS - Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)
* üîë Key:      
  * **Progressive** -> Easy2Hard
* üéì Source:
  * ECCV 2018 / Google AI      
* üå± Motivation:  
  * Progressive (Simple 2 Complex)
* üíä Methodology:
  * Cell-based Search Space 
  * NO RL or EA, Sequential-Model-Based Method
  * ÊØè‰∏ÄÊ≠•ÂÅö‰∏Ä‰∏™Â±ÄÈÉ®ÁöÑHeuristic SearchÔºå‰ª•Ââç‰∏ÄÊ≠•ÁöÑPredictorÊù•ÈÄâÂèñ‰∏ã‰∏ÄÊ¨°ÁöÑPredictor
* üìê Exps:       
* üí° Ideas:   



#### 7-A. [SNAS-Stochastic NAS](https://arxiv.org/pdf/1812.09926.pdf)

* üîë Key: 
  * **Gumble Softmax** in DARTS    
* üéì Source:      
  * ICLR2019 / SenseTime
* üå± Motivation:  
* üíä Methodology: 
* üìê Exps:       
* üí° Ideas:  



#### 10. [NAO-Neural Architecture Optimization](https://arxiv.org/abs/1808.07233)

* üîë Key: 
  * Arch **Encoder-Decoder**   
* üéì Source:      
  * NIPS2018 / MSRA
* üå± Motivation:  
  * Predictor-based
  * Ê∂âÂèä‰∫Ü‰∏Ä‰∏™ArchÁöÑEncoderÔºåÂ∞ÜarchÊò†Â∞ÑÂà∞‰∏Ä‰∏™ËøûÁª≠Á©∫Èó¥ÔºåÂú®ËØ•Á©∫Èó¥Áî®Gradient‰ºòÂåñ
* üíä Methodology:
    * Discrete -> Continuos 
      * ÂåÖÂê´‰∫Ü‰∏Ä‰∏™encoderÔºåÂ∞ÜarchÊò†Â∞ÑÂà∞‰∏Ä‰∏™ËøûÁª≠Á©∫Èó¥ÔºåÂêåÊó∂ËøòÊê≠ÈÖç‰∏Ä‰∏™decoder
      * ËøòÊúâpredictor 
    * ‰∏éÊ≠§Á±ª‰ººÁöÑÊòØDARTSÔºåËØ¥DARTSËÆ§‰∏∫ÊúÄÂ•ΩÁöÑarchÊòØÂΩìÂâçweight‰∏ãÁöÑargmaxÔºåËÄåNAOÁõ¥Êé•Áî®‰∏Ä‰∏™decoderÊò†Â∞ÑÂõûÊ®°Âûã
      * ËøòÊúâ‰∏ÄÊîØÊòØBayesian OptimizationÔºå‰ΩúËÄÖËÆ§‰∏∫GPÁöÑÊÄßËÉΩ‰∫éCovariance FunctionÁöÑËÆæËÆ°Âº∫Áõ∏ÂÖ≥
    * Search SpaceËÆæËÆ°
      * ‰∏§Ê≠•ÔºåÈ¶ñÂÖàÂÜ≥ÂÆö1Ôºâwhich 2 previous nodes as inputs 2)Á°ÆÂÆöË¶ÅÁî®‰ªÄ‰πàop
    * EncoderÂíåDecoderÈÉΩÊòØLSTMÔºåpredictorÊòØ‰∏Ä‰∏™mean-poolingÂä†mlp
    * ‰∏âËÄÖJointly Train
      * ËÆ§‰∏∫predictor could work as regularizationÂéªÈÅøÂÖçencoderÂè™ÂØπÂ∫îdecoderÁöÑÁªìÊûúÔºåËÄåÊ≤°ÊúâÊ≠£Â∏∏Ë°®ÂæÅ
        * Ëøô‰∏ÄÊ≠•Âíå‰º†ÁªüVAE‰∏≠ÁöÑÂä†noise‰∏ÄËá¥
    * ËÆ§‰∏∫weight-sharingÂíåNAOÊòØcomplementaryÁöÑ
* üìê Exps:       
* üí° Ideas:  
    * symmetricÁöÑdesign:‰∏∫‰∫Ü‰øùËØÅsymmetricÁöÑÊ®°ÂûãÔºàÂÆûÈôÖ‰∏äÊòØ‰∏Ä‰∏™Ê®°ÂûãÔºâÁöÑembedding‰∏ÄËá¥ÔºåpredictorÁªôÂá∫Â∑Æ‰∏çÂ§öÁöÑÁªìÊûú
      * Áî®‰∫ÜAugmentationÔºàflipÔºâÊù•ËÆ≠ÁªÉencoder









---


#### [Overcoming Multi-Model Forgetting in One-Shot NAS with DiversityMaximization](https://shiruipan.github.io/publication/cvpr-2020-zhang/)
* üîë Key:
  * Ëß£ÂÜ≥‰º†ÁªüÁöÑShared-WeightsÊñπÊ≥ïÂú®‰ºòÂåñÊñ∞ÁöÑÊû∂ÊûÑÁöÑÊó∂ÂÄôËÄÅÁöÑÊû∂ÊûÑÁ≤æÂ∫¶‰ºö‰∏ãÈôç(Catastrophic Forgetting)ÁöÑÈóÆÈ¢ò
* üéì SourceÔºö
  * CVPR 2020
* üå± Motivation: 
    * ‰º†ÁªüÁöÑOneShotÊñπÂºèËÆ§‰∏∫Jointly Optimized Supernet WeightsÊòØÊúÄ‰ºòÁöÑ
    * ‰ΩÜÊòØsequentially train archs with partially-shared weights‰ºöÂØºËá¥Catastrophic Forgetting
    * ÊñáÁ´†Ê†∏ÂøÉÊääOne-ShotNASÁúãÊàê‰∏Ä‰∏™Continual LearningÁöÑÈóÆÈ¢ò(Constrained Optimzation,learning of current arch should not degrade previous much)
* üíä Methodology:
    * NSAS(Search-based Architecture Selection) Loss Function 
    * Enforce the architectures inheriting weights from the supernet in current step perform better than last step
    * Â¶ÇÊûúÁ¥ØËÆ°ÁöÑËØùË¶ÅÊ±Ç‰ºöÂ§™È´ò‰∫ÜÔºåÊâÄ‰ª•‰∏çÊòØÈôêÂÆöÂÖ®ÈÉ®ÁöÑPrevious ArchÔºåËÄåÊòØÈÄâÊã©ÂÖ∂‰∏≠ÁöÑ‰∏Ä‰∏™Subset(ÂØπ‰∫éÂ¶Ç‰ΩïÈÄâÂÆöËøô‰∏™SubsetÊòØÂÅáÂÆöSubset‰∏≠ÁöÑArchË¶ÅÊúâDiversity-ÊâæÂà∞ÊúÄÂ§ßDiversityÁöÑËøáÁ®ãÂ∞±ÊòØÊâÄ‰∏∫ÁöÑNovelty Search)
    * ÂÆûÁé∞Á∫¶ÊùüÁöÑÊñπÂºèÊòØÂä†‰∏Ä‰∏™Soft Regularization


#### [One-Shot Neural Architecture Search via Self-Evaluated Template Network](https://arxiv.org/abs/1910.05733)
* üîë Key:
  * ‰º†ÁªüÁöÑEvluationÊÖ¢ÔºåShared WeightsÁöÑÊñπÂºèÈÄâÂèñÂéªEvaluateÁöÑÁªÑ‰ª∂ÁöÑÊó∂ÂÄôÊòØRandomÁöÑÔºå‰∏çÂ§üInstructive
* üéì SourceÔºö
  * ICCV 2019 / Xuanyi Dong, Yi Yang
* üå± Motivation: 
  * ÊèêÂá∫‰∫Ü‰∏Ä‰∏™SETN(Self Evaluated Template Network)
    * ‰∏Ä‰∏™EvaluatorÂéªÈ¢ÑÊµãÊúâÊõ¥‰ΩéValid LossÁöÑÊû∂ÊûÑ(Á±ª‰ºº‰∏Ä‰∏™Predictor)
    * ‰∏Ä‰∏™Ê®°ÊùøTemplateÁΩëÁªúÂéªShared Params,ÂåÖÂê´‰∫ÜÊâÄÊúâÁöÑCandidate
* üíä Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402210522.png)
    * Áúã‰∏äÂéªÂ∞±ÊòØÂØπShared-weightsÂä†‰∫Ü‰∏Ä‰∏™Predictor‰Ωú‰∏∫ControllerÔºåÂéª‰ªé‰∏Ä‰∏™ÊâÄË∞ìÁöÑTemplateÁΩëÁªú‰∏≠ÈááÊ†∑Âá∫Â≠êÊû∂ÊûÑÔºåControllerÊù•ÂÜ≥ÂÆöÊÄé‰πàÈáá(ËÄå‰∏çÊòØÈöèÊú∫ÈááÊ†∑)
  * N‰∏™Cell
    * ÊØè‰∏™Cell‰∏≠B‰∏™Block
    * ÊØè‰∏™BlockÂèØËÉΩÊòØ4ÂÖÉÁªÑ
  * Candidate Network 
    * Contain All candidate CNN in search space
    * train stochsticly - uniformly sample 1 candidate and only optimize its params
      * Optimize each with equal possibility
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402212418.png)
      * IÊòØinputÊòØÁ∫ØÈöèÊú∫SampleÔºåFÊòØFunctionÔºåÂÖ∂‰∏≠ÁöÑorderÊåáÁöÑÊòØÂÜç‰∏Ä‰∏™ÈõÜÂêàO‰∏≠ÈááÊ†∑ÔºåÂÖ∂‰∏≠f1ÁöÑindex‰∏ÄÂÆöË¶ÅÂ∞è‰∫éf2
    * EvaluatorÔºö
      * Encode one CNN candidate as a set of quadruples
      * ‰ªécategorical distribution sampleÂá∫‰∏Ä‰∏™choiceÔºåÁî®softmax normalized value‰Ωú‰∏∫vectorÂÄº
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402213049.png)
* üí° Ideas:
    * ÊåáÂá∫Êúâ‰∫õShared-WeightsÁöÑÊñπÊ≥ï‰ºöshared parameter with a learnable distribution of archs
      * ‰ΩúËÄÖËÆ§‰∏∫ËøôÊ†∑‰ºöÊúâbiasÔºåÂõ†‰∏∫Áõ∏ÂØπlightweightÁöÑmodel‰ºöÊõ¥Âø´Êî∂ÊïõÔºålearnable distribution will bias to these model
      * ‚Äúthe Matthew effect‚Äù to refer that some quickly-converged candidates will get more chances  to be further optimized in some NAS algorithms


#### [GATES]()
* üîë Key: 
  * NAS‰∏≠ÁöÑ**Predictor**ÈóÆÈ¢òÔºåÊèê‰æõ‰∏Ä‰∏™Êõ¥Â•ΩÁöÑEncoder
* üéì Source
  * Arxiv & THU EE
* üå± Motivation: 
  * Current Encoder model topological information implicitly
  * ÂéüÊú¨ÁöÑgcn‰πãÁ±ªÁöÑencoderÊñπÂºèedges stand for notion of affinity, feature on node
  * View NN as Data-Processing Graph
* üíä Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200404200848.png)
  * ËÆ§‰∏∫‰ºòÁÇπÂú®‰∫éËÉΩÂ§üÁõ¥Êé•Handle Topological isomorphism,‰ª•ÂèäÂØπInformation ProcessingÁöÑÂª∫Ê®°
* üìê Exps: 
  * Nb101/201‰∏äKDÔºåNatKÔºåPatK
* üí° Ideas:
  * Áõ∏ÂÖ≥ÊñáÁ´†NAOÔºåÁî®‰∏Ä‰∏™Encoder-DecoderÊù•ÂÅö
  * In the Kendall's Tau measure, all discordant pairs are treated equally
  * ÂÖ∂‰ªñÊåáÊ†á(Âõ†‰∏∫kendall tauÂÖ∂ÂÆûËÄÉËôë‰∫ÜÂæàÂ§öpoor archÁöÑÁõ∏ÂØπÂÖ≥Á≥ªÔºåÂØπ‰∫éNASÊù•ËØ¥Áî®Â§Ñ‰∏çÂ§ß)
    * NatKÔºö predictÁªôÂá∫ÁöÑK‰∏™‰∏≠ÊúÄÂ•ΩÁöÑÂÆûÈôÖrank
    * PatKÔºö Predictor TopKÂú®Gt TopK‰∏≠ÁöÑÊØî‰æã
  * Nb101 - 432k archs - Op on Node
  * Nb201 - 15625 - OP on Edge


---

### [How to Train Your Super-Net: An Analysis of Training Heuristics in Weight-Sharing NAS](https://arxiv.org/abs/2003.04276)
* üîë Key:         
  * Analysis of One-Shot NAS SuperNet Training
* üéì Source:      
  * Arxiv 2003
* üå± Motivation:  ÊïÖ‰∫ã
  * ÂØπ‰∫éSuperNetËÆ≠ÁªÉÁöÑ‰∏Ä‰∏™Ablation
  * ÈúÄË¶ÅÂ•ΩÂ•ΩË∞ÉÂèÇÔºå‰∏çÂ∫îËØ•Áî®Â∞ΩÈáèÂ∞ëÁöÑepochËÄåÂ∫îËØ•Áî®Subset
* üíä Methodology: ÊñπÊ≥ï
  * Áî®FairNAS‰Ωú‰∏∫Âπ≥Âè∞
  * **‰∏ªË¶ÅÈáçÁÇπÂú®‰∫éËÆ≠ÁªÉÂèÇÊï∞**(ÊòØÂê¶ÊúâAffineÔºåLRÔºåWD)
  * ËØÑ‰ª∑ÊåáÊ†áÊòØË∂ÖÁΩëÁªúÁöÑÂáÜÁ°ÆÁéáÔºåË∂ÖËøáÈöèÊú∫ÊêúÁ¥¢ÁöÑÊ¶ÇÁéá,ËøòÊúâkendall-tau„ÄÇ
  * ÊúÄÂêé‰∏Ä‰∏™ÊòØ‰ªéË∂ÖÁΩëÁªúÈöèÊú∫ÈááÊ†∑200‰∏™Êåë3‰∏™ÊúÄÂ•ΩÁöÑÔºåÂèñ‰ªñ‰ª¨ÁöÑground-truthÁöÑÂπ≥Âùá
* üìê Exps:        ÂÆûÈ™å
* üí° Ideas:       ÊÉ≥Ê≥ï


### [DetNAS: Backbone Search for Object Detection](http://arxiv.org/abs/1903.10979)
* üîë Key:   
  * nas 4 Det backbone
  * Supernet
* üéì Source:  
  * Megvii
* üå± Motivation: 
  * Det often needs imagenet pretraining & NAS requires accuracy as supervised signal
    * Imagenet-pretraining + Det finetune
  * Pre-training and finetuning are costly
    * Following One-shot, decouple the weight training and the architecture
  * Det task perf. as guide, to search for the Backbone Feature Extractor
* üíä Methodology:
    * Steps: 
      1. SuperNet Pretrain on ImageNet and finetune on Det Task
      2. NAS on Supernet with EA
        * Path-wise(at one time, only updating samples path)
    * Finetuning BN
      * Freezing BN(Traditional) wont work for supernet(normalize couldnt acquired at different paths)
      * SyncBN replace regular BN
        * Compute BN Statistics across multiple GPUs (save memory consumption)
      * Also when EA searches arch, each BN param should be independent
        * need to re-accumulate the BN for every new arch
    * SS Design
      * Small/Big (40/20 Blocks)
        * Small used in Ablation Study
      * based on ShuffleNetV2 Block(involves channel split and shuffle operation)
        * 4 Choices:
        * x3: kernel-size [3,5,7]
        * x1: replacing right branch with Xception block(3 repeated DW 3x3 Conv)
        * 4^(40) choices for big ss
    * EA
      * Mutation + CrossOver
      * arch dont meet constraint will be removed when updating
* üìê Exps: 
* üí° Ideas:
  * Det's Direction
    * Architecture: FPN - Top-Down arch with lateral connection, integrating features at all scales
    * Loss: RetinaNet's Focal Loss, dealing with the class imbalance(Instability in earlier training?)
    * MetaAnchor: dynamic anchor mechanism
  * AmeobaNet - plain EA without Controller could also achieve
  * Det often uses the image classification backbone which could be sub-optimal(DetNet59 > ResNet101)
  * NAS-FPN only searches for the Feature Pyramid Network
  * EA could better handle Constraints than RL & Gradient-based 
  * [SyncBN](https://tramac.github.io/2019/04/08/SyncBN/)
    * Plain BN with DataParallel - Input distiributed to subsets and build different models on different GPU
    * (Since independent for each GPU, so batch size actually smallen)
    * Key2SyncBN: Get the global mean & var
      * Sync 1st then calc the mean & var
      * (Simplest implemention is first sync together calc mean, then send it back to calc var) - Sync twice
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418111031.png)
        * modify computation flow and only sync once
    * [Code](https://github.com/tamakoji/pytorch-syncbn)
  * ------------------------------------------
  * Rather Small SS
  * Simple Shared-Weights with interesting handle of BN


### [NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](http://arxiv.org/abs/1904.07392)
* üîë Key:   
  * Search a FPN(Feature Pyramid Network) in a ss covering all cross-scale connections
* üéì Source:
  * Quoc V Le Google  
* üå± Motivation: 
  * Huge design space(increase exponentially)
* üíä Methodology:
  * Following Cell-based SS(author called it as scalable architecture), main contribution is designing **search space**
    * the SS is **modular**  (Repeat the FPN N times then concat into a large net)
  * RNN RL Controller
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418171859.png)
  * Feature Pyramid Network
    * (following RetinaNet, use last layer in each group of feature map as *the input of the FPN*)
    * 5 Scales(C_{3,4,5,6,7}) stride of 8/16/32/64/126 pixelÔºà6,7 purely max-pooling of 5Ôºâ for Merging Cell
    * Composed of multiple merging cells  
    * Input/Output same size - can be stacked (num to stack would control the acc/flops trade-off)
    * Each merging cell gives output are appended into the candidate layers, also feeds into next merging cell
      * Finally the 5 merging cells are output 
  * Merging Cell
    * basic element of FPN
    * Merging 2 input feature map of different size
    * Each Cell has its own resolution(the output)
    * 4 Step: 
      1. choose input-layer-1 
      2. choose input-layer-2 
      3. choose output feature resolution
      4. select binary op(add or maxpool) - (scales are handles b4 the binary op)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173400.png)
  * Meshgrid Representation
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173458.png)
* üìê Exps:
  * Scalable: Stacking NAS-FPN blocks could improve acc while stacking simple block couldnt
* üí° Ideas:
  * FPN - fuse features across different scales (cross-scale connection in ConvNets)
    * Deeper layer feature semantically strong but less resolution, are upsampled and added with lower-representation for feature with both good semantic and resolution  
    * sequentially combining 2 adjacent layer feature (with top-down/lateral connection)
    * RW
      1. [Path aggregationnetwork for instance segmentation. In CVPR, 2018.]() - add an additional bottom-up pathway
      2. [M2det: A single-shot object detector based on multi-level feature pyramid network. AAAI, 2019.]() - Multiple U-shaped Modules
      3. [Deep feature pyramid reconfiguration for object detection. In ECCV, 2018]() - Combine features at all scale + Global attention
    * Problem: manually designed and shallow(compared to backbone)
  * Any-time detection: dont necessarily need to forward all pyramid networks
    * Desired when computation effort is concern
  * ----------------------------------------------------------------------------------
  * Google's Work, really strange Hyper-paramÔºàlr-0.08/8 epochs trainingÔºâ (Maybe Grid-Search Again?)


### [EfficientDet: Scalable and Efficient Object Detection](http://arxiv.org/abs/1911.09070)
* üîë Key:   
	* Systematically NAS for Det Task
  * Combining EfficientNet Backbone + Bi-FPN + Compound Scaling
* üéì Source:  
	* Quo V Le Google Brain
* üå± Motivation: 
	* Weighted Bi-directional FPN - for multi-scale feature fusion (Better Feature Aggregation)
	* Compound Scaling method - uniformly scale the resolution/depth/width (Scalable)
* üíä Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418223126.png)
  * the "BiFPN", analogy of FPN & PANet
    * from traditional "Top-down" structure(1-way information flow)
    * Adding an extra edge when I/O is at the same level
    * remove node with only one input edge 
    * Adding Weighted Feature Fusion - like Attention
    * exponential scale up BiFPN width
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418220047.png)
* üìê Exps:
* üí° Ideas:
  * One-Stage Det: (Anchor-Free) whether have a region proposal step









## Reference

* [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)
* [AutoML.org](https://www.automl.org/automl/literature-on-neural-architecture-search/)

 
