# Awesome-Neural-Architecture-Search

> ä¾¿äºä¸ªäººæŸ¥æ‰¾çš„ä¸€äº›NASæ–‡ç« çš„æ¢³ç†ä»¥åŠç®€è¦Digest
> é‡‡ç”¨äº†ä¸[Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)ä¸åŒçš„é€æ¨¡å—   æ¢³ç†æ–¹å¼ï¼Œä¾¿äºä¸ªäººç†è§£ä¸é€ŸæŸ¥

## Paper List

> æœ‰ç¼–å·çš„æ–‡ç« æ˜¯æ—©æœŸæ¯”è¾ƒé‡è¦çš„æ–‡ç« ï¼ŒFollowå…¶çš„æ–‡ç« ï¼Œä¼šç”¨9-Açš„æ ¼å¼æ¥æ ‡æ³¨

|Title ğŸ“•|Source ğŸ“|Code ğŸ’»|Component ğŸ”¨|Property ğŸ’ |
|--|--|--|--|--|
|[1. Neural Architecture Search with Reinforcement Learning](https://arxiv.org/pdf/1611.01578)|ICLR2017(1611) *Zoph.* at Google Brain| - | Flow |NAS Flow|
|[2. Accelerating Neural Architecture Search Using Performance Prediction](https://arxiv.org/pdf/1705.10823)|ICLR2018W(1705) *Baker* at MIT| - |Evaluator|Predictor-based Evaluator|
|[3. eNAS - Efficient Architecture Search by network transformation](https://arxiv.org/abs/1707.04873)|AAAI2018(1707) *Cai* at SJTU|-|Flow/Weights-Manager|Shared Weights/Mutation from Existing Network/RL Controller|
|[4. Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012.pdf)|CVPR2018(1707) *Zoph* Google Brain|-|Search Space|Cell-based Search Space|
|[5. HyperNet - SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)|ICLR2017 *Brook*|-|Weights-Manager/Evaluator|HyperNet 2 Produce SubNet's Weight|
|[5-A. Graph HyperNetwork for Neural Architecture Search](https://arxiv.org/abs/1810.05749)|ICLR2019 *Chris Zhang* Toronto|-|Weights-Manager/Evaluator|HyperNet 2 Produce SubNet's Weight|
|[6. ENAS - Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)|ICML2018(1802) *Pham* (at) Google Brain|-|Flow/Weight-Manager/Evaluator|Shared Weights Flow|
|[7. DARTS - Differentiable Architecture Search](https://arxiv.org/pdf/1806.09055)|ICLR2019(1806) *Liu* (at) Google Brain|-|Flow/Controller|Gradient-based Flow|
|[7-A. SNAS - Stochastic Architecture Search](https://arxiv.org/pdf/1812.09926)|ICLR2019(1812) *Xie* (at) SenseTime |-|Controller|Gradient-based Flow|
|[7-B. DARTS-nds - On Network Design Spaces for Visual Recognition](https://arxiv.org/pdf/1905.13214.pdf)|ICCV2019(1905) *IIija* (at) FAIR |-|SS|Improved NAS SS|
|[8. Hierarchical Representations for Efficient Architecture Search](https://arxiv.org/pdf/1711.00436)|ICLR2018(1711) *Liu* (at) Google Brain|-|Search Space|Hierarchical SS|
|[9. Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)|ECCV2018(1712) *Liu* (at) Google AI|-|Controller|Predictor-based/Easy2Hard|
|[10. NAO - Neural Architecture Optimization](https://arxiv.org/abs/1808.07233)|NIPS2018(1808) *Luo* (at) MSRA|-|Evaluator|Predictor-based/Gradient-based|
|-----------------Det------------------------|-----------------|-----|--------------------|--------------------|
|[DetNAS: Backbone Search for Object Detection](http://arxiv.org/abs/1903.10979)|Arxiv(1903) *SunJian* at Megvii|-|Task|Shared-Weights4DetBackbone|
|[NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](http://arxiv.org/abs/1904.07392)|Arxiv(1904) *Quo V Le* at Google Brain|-|Task/Search Space|Search for FPN|
|[EfficientDet: Scalable and Efficient Object Detection](http://arxiv.org/abs/1911.09070)|Arxiv(1911) *Quo V Le* at Google Brain|-|Task/Search Space|BiFPN+Weighted+Scalable Arch|
|-----------------Binary------------------------|-----------------|-----|--------------------|--------------------|
|[Binarizing MobileNet via Evolution-based Searching](http://arxiv.org/abs/2005.06305)|Arxiv(2005)|-|Task|Evo Search for group-conv MobileBlock|
|[Searching for Accurate Binary Neural Architectures](http://arxiv.org/abs/1909.07378)|ICCVW19(1909) Huawei Noah|-|Task|Evo Search width for MobileBlock|
|[Learning Architectures for Binary Networks](http://arxiv.org/abs/2002.06963)|ECCV2020(2002) GIST|-|Task|Darts+Binary|
|[Binarized Neural Architecture Search](http://arxiv.org/abs/1911.10862)|AAAI2020(1911) Beihang|-|Task|Darts+Binary|
|[CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks](http://arxiv.org/abs/2005.00057)|CVPR2020(2005) Beihang|-|Task|Darts+Binary+Tch/Stu|
|[BATS: Binary ArchitecTure Search](http://arxiv.org/abs/2003.01711)|ECCV2020(2003) Cambridge|-|Task|Darts+Binary|
|-------------------Mixed---------------------|-----------------|-----|--------------------|--------------------|
|[A Survey on Neural Architecture Search](https://arxiv.org/pdf/1905.01392.pdf)|Arxiv(1905) *Martin* at IBM|-|Survey|-|
|[Accelerator-Aware Neural Network Design Using AutoML](https://arxiv.org/abs/2003.02838)|MLsys20-W Gupta|-|Hardware|NAS4Accelerator|
|[MTL-NAS: Task-Agnostic Neural Architecture Search towards General-Purpose Multi-Task Learning](https://arxiv.org/abs/2003.14058)|CVPR2020 Gao|-|Flow|NAS + MultiTasking|
|[GreedyNAS: Towards Fast One-Shot NAS with Greedy Supernet](https://arxiv.org/abs/2003.11236)|CVPR2020 You|-|Weights-Manager/Evaluator|Improvement of HyperNet|
|[EcoNAS: Finding Proxies for Economical Neural Architecture Search](https://arxiv.org/abs/2001.01233)|CVPR2020 Dong.|-|Weights-Manager|Evaluate Proxy|
|[Improved one-shot NAS by suppressing posteriror fading](http://xxx.itp.ac.cn/pdf/1910.02543v1)|CVPR2020 Li|-|Weights-Manager|Bayesian+One-shot|
|[How to Train Your Super-Net: An Analysis of Training Heuristics in Weight-Sharing NAS](https://arxiv.org/abs/2003.04276)|Arxiv(2003)|-|One-Shot|Analysis for Training One-Shot|
|[Disturbance-immune Weight Sharing for Neural Architecture Search](https://arxiv.org/abs/2003.13089)|Arxiv(2003) Niu|-|One-Shot|Improve Weight -Sharing|
|[NPENAS:Neural Predictor Guided Evolution for Neural Architecture Search](https://arxiv.org/abs/2003.12857)|Arxiv(2003) Wei|-|Predictor|Predictor+Evo|
|[DA-NAS:Data Adapted Pruning for Efficient Neural Architecture Search ](https://arxiv.org/abs/2003.12563)|Arxiv(2003) Dai|-|Flow|Speed up NAS flow via triming blocks|
|[MiLeNAS: Efficient Neural Architecture Search via Mixed-Level Reformulation](https://arxiv.org/abs/2003.12238)|Arxiv(2003) He|-|Gradient-based Flow|Bi-level Optimization lead to Sub-optimal|
|[Are Labels Necessary for Neural Architecture Search? ](https://arxiv.org/abs/2003.12056)|Arxiv(2003) Liu & Kaiming|-|Evaluator|Unsupervised Learning for NAS Evaluator|
|[Sampled Training and Node Inheritance for Fast Evolutionary Neural Architecture Search](https://arxiv.org/abs/2003.11613)|Arxiv(2003) Zhang|-|Controller|Faster EVO|
|[BigNAS: Scaling Up Neural Architecture Search with Big Single-Stage Models](https://arxiv.org/abs/2003.11142)|Arxiv(2003) Yu|-|Shared-Weights|No Retraining Weight(like OFA)|
|[PONAS: Progressive One-shot Neural Architecture Search for Very Efficient Deployment](https://arxiv.org/abs/2003.05112)|Arxiv(2003) Huang|-|Evaluator|Progressive+One-shot|
|[Steepest Descent Neural Architecture Optimization: Escaping Local Optimum with Signed Neural Splitting](https://arxiv.org/abs/2003.10392)|Arxiv(2003) Wu|-|Evaluator|Improved NAO Flow,escape Local minima|
|[Real-time Federated Evolutionary Neural Architecture Search](https://arxiv.org/abs/2003.02793)|Arxiv(2020) Zhu|-|Flow|Evo+Federated Learning|
|[ADWPNAS: Architecture-Driven Weight Prediction for Neural Architecture Search](https://arxiv.org/abs/2003.01335)|Arxiv(2020) Zhang|-|Evaluator(HyperNet)|HyperNet Improve|
|[BS-NAS: Broadening-and-Shrinking One-Shot NAS with Searchable Numbers of Channels](https://arxiv.org/abs/2003.09821)|Arxiv(2003) Shen|-|Evaluator/Weights-Manager|Controllable Shared-Weights|
|[Hit-Detector: Hierarchical Trinity Architecture Search for Object Detection](https://arxiv.org/abs/2003.11818)|Arxiv(2003) Guo|-|Task|HierNAS + Det|
|[DCNAS: Densely Connected Neural Architecture Search for Semantic Image Segmentation](https://arxiv.org/abs/2003.11883)|Arxiv(2003) Zhang|-|Task|NAS4Semantic Seg|
|[Probabilistic Dual Network Architecture Search on Graphs](https://arxiv.org/abs/2003.09676)|Arxiv(2003) Zhao|-|Task|NAS4GNN|
|[GAN Compression: Efficient Architectures for Interactive Conditional GAN](https://arxiv.org/abs/2003.08936)|Arxiv(2003) Li|-|Task|NAS4GAN|
|[ElixirNet: Relation-aware Network Architecture Adaptation for Medical Lesion Detection](https://arxiv.org/abs/2003.08770)|Arxiv(2003) Jiang|-|Task|NAS+Medical|
|[Lifelong Learning with Searchable Extension Units](https://arxiv.org/abs/2003.08559)|Arxiv(2003) Wang|-|Task|NAS+Continual Learning|
|[Hierarchical Neural Architecture Search for Single Image Super-Resolution](https://arxiv.org/abs/2003.04619)|Arxiv(2003) Guo|-|Task|HierNAS4SR|
|[NAS-Count: Counting-by-Density with Neural Architecture Search](https://arxiv.org/abs/2003.00217)|Arxiv(2020) Hu|-|Task|Counting|








## Paper Digest


```
---------------Format---------------
* ğŸ”‘ Key:         
* ğŸ“ Source:      
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:       
-----------------------------------
```


#### 1. [NAS with RL](https://arxiv.org/pdf/1611.01578.pdf) - Google Brain
* ğŸ”‘ Key:
  * NAS Work Flow
* ğŸ“ Sourceï¼šICLR2017 / Google Brain
* ğŸŒ± Motivation:
  * é¢†åŸŸå¼€ç«¯ï¼ŒNASçš„å¼€å±±,å¥ å®šäº†å¤§è‡´Workflow
  * å»ºç«‹åœ¨è®¤ä¸ºNNçš„connectivityå’Œstructureå¯ä»¥è¢«è¡¨è¾¾æˆä¸€ä¸ªvariable-length stringï¼ˆå¯ç”¨RNNåšembeddingï¼Œä¹Ÿå°±æ˜¯ä½œä¸ºcontrollerï¼‰
    * å°†é‡‡æ ·ï¼ˆRNNç”Ÿæˆï¼‰å‡ºæ¥çš„å­æ¶æ„è¿›è¡Œè®­ç»ƒï¼Œå°†Accä½œä¸ºrewardä¿¡å·ï¼Œé€šè¿‡Policy Gradientçš„æ–¹å¼å›ä¼ å»update controllerï¼ˆOriginal RNNï¼‰
* ğŸ’Š Methodology: 
  * search space
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203201724.png)
  * RNN
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203202126.png)
    * å°†RNNçœ‹ä½œä¸€ä¸ªæ ‘çš„ç»“æ„
* ğŸ“ Exps: 
  * æ¯”cifar10ä¸Šçš„baselineæäº†0.1ä¸ªç‚¹ï¼Œå¹¶ä¸”å¿«äº†ä¸€ä¸¢ä¸¢1.05ï¼Œä»¥åŠå…¶ä»–çš„dataset-PennTreebankä»¥åŠlanguage modelingçš„task
* ğŸ’¡ Ideas:
  * åœ¨relatedworkä¸­è®²åˆ°
    * hyper-param optimizationåªèƒ½å¤Ÿåšåˆ°åœ¨fixed-lengthçš„spaceè¿›è¡Œæ¨¡å‹ä¼˜åŒ–ï¼Œä¸”å¯¹good initial modelæ¯”è¾ƒä¾èµ–
    * Bayesianæ–¹æ³•å¯ä»¥å¯»æ‰¾ä¸€ä¸ªâ€œä¸å®šé•¿â€çš„æ¶æ„ï¼Œä½†æ˜¯ä¸æ˜¯å¾ˆæœ‰æ„ä¹‰
  * The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning. 


#### 2. [Accelerating NAS using performance prediction](https://arxiv.org/pdf/1705.10823.pdf)

* ğŸ”‘ Key: 
  * **Predictor**        
* ğŸ“ Source: ICLR2018W / CMU   
* ğŸŒ± Motivation:  
  * **æå‡ºpredictoræ¥åŠ é€Ÿevaluation**,ä»¥åŠEarlyStopping
  * è®¤ä¸ºhumanæ˜¯ä»training curveæ¥è§‚å¯Ÿçš„ï¼Œæœ¬æ–‡parameterizeäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œè®­ç»ƒregression modelæ¥é¢„æµ‹acc
  * å¯¹åº”çš„counterpartæ˜¯Bayesiançš„ä¸€äº›æ–¹æ³•
    * ç›®æµ‹æ˜¯äººå·¥è®¾è®¡ä¸€äº›æ‹Ÿåˆlearning curveçš„base functionï¼Œç„¶åç”¨expensiveçš„MCMCæ¥æ‹Ÿåˆ
    * ä¹Ÿæœ‰ç”¨é«˜æ–¯è¿‡ç¨‹æ ¸å‡½æ•°çš„
* ğŸ’Š Methodology: 
  * è¯´ç™½äº†å»ºæ¨¡çš„æ˜¯Training Curve
    * è¾“å‡ºval accï¼Œè¾“å…¥æ˜¯configurationï¼Œåœ¨æ¯ä¸ªtime step
    * åšäº†ä¸€ä¸ªSRMï¼ˆSequential Regression modelï¼‰æ¯”å¦‚RBMï¼ŒRandomForestç­‰
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203204919.png)
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:      



#### 3. [eNAS - Efficient Architecture Search by Network Transformation](https://arxiv.org/pdf/1707.04873.pdf)

* ğŸ”‘ Key:  
  * **Weight Manager** -> Shared Weights
* ğŸ“ Source:  AAAI2018 / SJTU
* ğŸŒ± Motivation:  
  * **Reusing Weight(Weight Manager)ä¸éœ€è¦from scratchæ¥è®­ç»ƒç½‘ç»œ**
  * metacontrolleré‡‡æ ·æ¶æ„based on Network transformation 
* ğŸ’Š Methodology: 
    * ä¾ç„¶æ˜¯Rl Controller - Policy Gradientæ›´æ–°ï¼Œå…¶actionæ˜¯æŒ‡å¯¼transformationï¼Œç½‘ç»œçš„ç»“æ„encoderä¾ç„¶æ˜¯ä¸€ä¸ªBi-LSTM
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190254.png)
      * æœ‰ä¸¤ç§Actorå½¢å¼ï¼ŒNet2Wideræˆ–è€…Net2Deeper
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190452.png)
      * Widerç”¨ä¸€ä¸ªå…¬ç”¨çš„sigmoidåˆ†ç±»å™¨æ¥ç”Ÿæˆåé¦ˆä¿¡æ¯ï¼Œç¡®å®šæ˜¯å¦åœ¨è¯¥æ—¶åˆ»éœ€è¦expand
      * Net2Deeperåˆ™æ˜¯æ˜¯å¦éœ€è¦insertä¸€ä¸ªæ–°çš„layer
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190710.png)
      * Deeperç”¨ä¸€ä¸ªRNNæ¥å»ºæ¨¡æ˜¯å¦éœ€è¦åœ¨æŸä¸ªä½ç½®æ’å…¥ä¸€ä¸ªæ–°å±‚
      * å¯ä»¥æ’å…¥æ–°å±‚çš„ä½ç½®æ˜¯é¢„å…ˆå›ºå®šçš„ï¼Œä¾æ®poolingå±‚çš„ä½ç½®æŠŠç½‘ç»œåˆ†æˆå‡ ä¸ªblock
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:   
    * (ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯muation-basedï¼Œä¸è¿‡æŒ‡å¯¼mutationäº§ç”Ÿçš„ä¸æ˜¯é—ä¼ æˆ–è€…SAè¿™ç±»heuristicçš„æ–¹æ³•ï¼Œä½†æ˜¯æ˜¯RL-Agent)


#### 4. [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012.pdf)
* ğŸ”‘ Key:      
  * **Cell-based Search Space** (NASNet SS)   
* ğŸ“ Source:     
  * CVPR2018 / Google Brain 
* ğŸŒ± Motivation:  
  * å°†SearchSpaceåˆ†æˆå¤šä¸ªç›¸åŒCellçš„å †å 
* ğŸ’Š Methodology:
  * Convolutional Blocks Repeated many times
    	* ä¸€èˆ¬åˆ†æˆNä¸ªNormal Cellhåé¢æ¥ä¸€ä¸ªReduction Cell
  * åŸºäºRNNçš„Controller
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185419.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185525.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185626.png) 
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:    
	* Regularization Technique - Schedule Drop Path



#### 5. [SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)

* ğŸ”‘ Key:  
  * **HyperNet** -> Produce Weight
  * **One-shot NAS Framework** -> Shared Weights      
* ğŸ“ Source:  
  * ICLR2017 
* ğŸŒ± Motivation: 
  * **one-shotè¡¨ç¤ºä»ä¸€ä¸ªHyperNetä¸­å–å‡ºæ¶æ„ï¼Œä¸éœ€è¦trainingä½œevaluationï¼Œé‡‡ç”¨weight sharing** 
  * HyperNetç”¨æ¥å¯¹ç»™å®šçš„æ¶æ„ç”ŸæˆWeightsï¼Œåšåˆ°Fast Eavluation
* ğŸ’Š Methodology: 
   * HyperNet - Training an auxiliary HyperNet to generate weights
     * åŠ é€Ÿarch selection
     * ä»binary codedåˆ°optimal architecture weightsçš„mappingï¼Œåªè®­ç»ƒoutput layer(?)
     * å…¶è®­ç»ƒæ˜¯ç”¨gradient-basedçš„æ–¹å¼æ¥åšåˆ°çš„
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:      
   * é€‰å–archæ¥evluateçš„æ–¹å¼
     * Memory-bank viewï¼Œå°†å…¶ä½œä¸ºbinary vector
     * æ˜¯å¦æ„å‘³ç€ä¼šéå†æ•´ä¸ªsearch spaceï¼Ÿ
   * æ–‡ç« ä¸­æåˆ°äº†è¯´*è®­ç»ƒä¸€ä¸ªarchå¼€å§‹çš„éƒ¨åˆ†æ˜¯æ•´ä½“accçš„ä¸€ä¸ªinsight*
     * ä¸€èˆ¬çš„æ–¹æ³•æŠŠarchçš„perfçœ‹ä½œä¸€ä¸ªblack boxï¼Œç”¨BOæˆ–è€…rså»æœç´¢ï¼Œä¹Ÿæœ‰early-stoppingè¿™æ ·çš„ç­–ç•¥ 
   * èƒ½å¤ŸæŠ›å¼ƒå…¶ä»–æ‰€æœ‰çš„hyper-paramä»¥åŠdynamic-regulariaztionçš„ä¸œè¥¿
     * æ¯”å¦‚lr schedule
     * æ¯”å¦‚DropPathä¹‹ç±»çš„ä¸œè¥¿
   * [Meta-Pruning(ICCV2019)](https://arxiv.org/pdf/1903.10258)å’Œè¿™ä¸ªæœ‰ç‚¹åƒçš„
   * We hypothesize that so long as the HyperNet learns to generate reasonable weights, the validation
error of networks with generated weights will correlate with the performance when using normally
trained weights



#### 5-A. [Graph HyperNetwork for Neural Architecture Search](https://arxiv.org/pdf/1810.05749.pdf)
* ğŸ”‘ Key:  
  * å›¾å½¢å¼çš„Hypernet
* ğŸ“ Source:  
  * ICLR 2019 && Uber & Toronto Univ.
* ğŸŒ± Motivation: 
  * 1st to Generate All Weights
  * å¤šç§HyperNetçš„æ–¹å¼ï¼š
    * 3D Encoding Tensor
    * LSTM Process a Sequence
* ğŸ’Š Methodology: 
  * SS
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413183600.png)
  * GNN(ç”ŸæˆGraph Embedding)æ¯ä¸ªNodeæ˜¯ä¸€ä¸ªLSTMï¼Œç”Ÿæˆä¸€ä¸ªEmbedding
  * HyperNetå°±æ˜¯ä¸€ä¸ªMLPï¼Œå¯¹æ‰€æœ‰Nodeå…¬ç”¨ï¼Œè¾“å‡ºå¤§å°å›ºå®š
    * å¯¹äºä¸åŒå¤§å°çš„Weightï¼Œç”¨å †å Kernelæˆ–è€…æ˜¯Channelè¾¾æˆ
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413183534.png)
  * *AnyTime Prediction* *Forward Backward Pass*  
    * æ›´æ–°çš„æ­¥éª¤ä»¿ç…§ç½‘ç»œå‰å‘ä»¥åŠåå‘çš„æ­¥éª¤
    * TimeStep 2V-1
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200413182757.png)
* ğŸ’¡ Ideas:     
  * æ‰€è°“Motifçš„æ–¹å¼æ›´ä¸ºé«˜æ•ˆï¼Œæœç´¢ä¸€ä¸ªCellï¼Œå¯¹äºæ–°çš„ä»»åŠ¡ï¼Œå°±æ”¹å˜Cellçš„å †å æ¨¡å¼å°±å¯ä»¥äº†

#### 6. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)

* ğŸ”‘ Key:
  * **SuperNet**
  * **One-Shot** -> Shared Weightsï¼ˆCurrent Popular Flowï¼‰     
* ğŸ“ Source:      
  * ICML 2018 / Google Brain
* ğŸŒ± Motivation:
  * æ„å»ºä¸€ä¸ªSupernetï¼Œè®¤ä¸ºæ‰€æœ‰æ¶æ„éƒ½æ˜¯è¿™ä¸ªSuperNetçš„Computation Graphçš„ä¸€ä¸ªSubGraphï¼Œå„ä¸ªChild Module Share Params  
* ğŸ’Š Methodology:
   * Controlleråœ¨ä¸€ä¸ªåºå¤§çš„Compuation Graphä¸Šæœç´¢Subgraphä½œä¸ºå­å›¾
    * å¯¹äºchild modules Share Parameter  
  * RNN Controller
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308232519.png)
      * é‡‡æ ·å‡ºé“¾æ¥å…³ç³»
      * ä»¥åŠnodeæ˜¯ä»€ä¹ˆ
    * è¿™ä¸ªcontrolleræ˜¯æ€ä¹ˆè®­ç»ƒçš„ï¼Ÿ
      * Policy Gradientï¼Ÿ
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  



#### 7. [DARTSï¼šDifferentiable Architecture Search](https://arxiv.org/pdf/1806.09055.pdf)

* ğŸ”‘ Key:     
  * **ç¦»æ•£æœç´¢ -> Gradient-based**    
* ğŸ“ Source:   
  * ICLR 2019 / Google Brain   
* ğŸŒ± Motivation: 
  * å°†åŸå…ˆçš„ç¦»æ•£æœç´¢ï¼Œæ”¹ä¸ºDifferentiableæ–¹å¼(Relax the Search space to be Differentiable),ä»¥æé«˜æ•ˆç‡ï¼
  * æ ¸å¿ƒåœ¨äºå¦‚ä½•ä¿®æ”¹Search Space 
* ğŸ’Š Methodology: 
  * Search Spaceå¦‚ä½•è®¾è®¡
    * ä¸€ä¸ªCellæ˜¯ä¸€ä¸ªDAGï¼ŒNodeæ˜¯ä¸€ä¸ªlatent representation(ä»£è¡¨Feature Map)ï¼Œæ¯ä¸ªæœ‰å‘çš„è¾¹å’ŒæŸç§æ“ä½œ(Op)æœ‰å…³
      * æ¯ä¸ªopä»¥softmaxæ¥relaxï¼Œå–å„ä¸ªå®é™…æ“ä½œ(Max-pool/Conv/No)å‘ç”Ÿçš„æ¦‚ç‡
    * è®¤ä¸ºæ¯ä¸ªcellæœ‰ä¸¤ä¸ªè¾“å…¥ä¸€ä¸ªè¾“å‡ºï¼Œå¯¹å·ç§¯å±‚è¾“å…¥æ¥è‡ªprevious 2 layer
      * cellä¸€å®šæœ‰ä¸¤ä¸ªè¾“å…¥nodeæ¥è‡ªäºå‰ä¸¤ä¸ªblockçš„è¾“å‡º
    * N=7 Node
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807192915.png)
  * Relaxation
    * just a softmax
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807183907.png)
  * important approximation
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807192038.png)
    * approximate the inner optimization(train the subnet until convergence), with just one step gradient optimization 
  * Bi-level Optimization
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807184006.png)  
    * Learn Arch and Weight at the same time
    * alphaçš„å¯¼æ•° \partial{L_val}/\partrial{\alpha}
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312222701.png)
      * åšäº†ä¸€ä¸ªapproxåˆ°O(alpha*w),åˆ°äº†O(alpha+weight)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312221224.png)
* ğŸ“ Exps:  
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807193002.png)     
* ğŸ’¡ Ideas: 
	* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/0401b088f9f6c5f80f137d093a29c4d.png)
		* æ³¨æ„deriveçš„æ—¶å€™å¯¹æ¯ä¸ªnodeé€‰æ‹©æœ€å¤§çš„2ä¸ªedge
	* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200808171250.png) 

#### 7-1. [P-Darts:Progressive Differentiable Architecture Search: Bridging the Depth Gap between Search and Evaluation](http://arxiv.org/abs/1904.12760)
* ğŸ”‘ Key:   
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807195111.png)
  * é€æ¸å¢åŠ evalæ·±åº¦ä»¥è§£å†³depth gap![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200807195111.png)  
* ğŸ“ Source: 
  * Huawei Noah & Tongji
* ğŸŒ± Motivation: 
  * åŸæœ¬çš„dartsåœ¨8ä¸ªcellçš„æƒ…å†µä¸‹åšè®­ç»ƒå¹¶ä¸”deriveå‡ºarchï¼Œä½†æ˜¯æœ€åå¯¼å‡ºæ¨¡å‹çš„æ—¶å€™ç”¨çš„æ˜¯20ä¸ªcellçš„æ¨¡å‹
* ğŸ’Š Methodology:
  * æ­£å¸¸çš„depth growthä»¥åŠä¸€ä¸ªprogressive pruningï¼Œé€æ¸pruneè¢«é€‰åˆ°æ¦‚ç‡ä½çš„op
    * åŠ äº†ä¸€ä¸ªsearch space regularization,ä¹Ÿå°±æ˜¯operational level dropout
    * ä¸»è¦æ˜¯ä¸ºäº†è§£å†³æœç´¢è¿‡ç¨‹ä¼šé€æ¸è¶‹å‘shortcut
      * ä»¥åŠå¼ºåˆ¶restrict num of shortcut
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 
  * Path dropoutçš„å…·ä½“æ“ä½œæ–¹å¼ï¼ŸæŸæ¬¡sampleçš„æ—¶å€™ä»¥0.1çš„æ¦‚ç‡æŸä¸ªedgeä¸è¿æ¥ï¼Ÿå¤±æ•ˆ(é‚£æœ‰æ²¡æœ‰å¯èƒ½éƒ½å¤±æ•ˆäº†)

#### 7-2. [PC-DARTS: Partial Channel Connections for Memory-Efficient Architecture Search](http://arxiv.org/abs/1907.05737)
* ğŸ”‘ Key:   
  * æ ¹æœ¬åŸå› æ˜¯ï¼šå‡è®¾é”™è¯¯
    * å¥½çš„supernetä¸ä¸€å®šä»£è¡¨ç€å¥½çš„å­ç½‘ç»œ
  * æ­£å¸¸è®­ç»ƒä¼šå¯¼è‡´weight over-fittingäºsupernet(?)
  * è§£å†³æ–¹æ¡ˆï¼š éšæœºé‡‡æ ·éƒ¨åˆ†çš„supernet channel
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
  * è¡¨ç¤ºä¸åŒopçš„åŒºåˆ«åœ¨ä¸€ä¸ªbatchçš„æ•°æ®ä¸Šä½“ç°çš„ä¸æ˜æ˜¾ï¼Œéœ€è¦åŠ å¤§batchï¼Œå°±éœ€è¦å‡å°memoryï¼Œæ‰€ä»¥å°±éœ€è¦åŠ å¤§bs
* ğŸ’Š Methodology:
  * partial channel sampling
    * sampleå‡ºä¸€éƒ¨åˆ†channelï¼Œå‰©ä¸‹çš„channelåœ¨è®¡ç®—ä¸­è¢«bypasså¹¶ä¸”ç›´æ¥æ¥åˆ°outputä¸Š(ç±»ä¼¼äºshortcutçš„å½¢å¼)
    * ä¼˜åŠ¿ï¼š
      * è®¤ä¸ºåªç”¨ä¸€éƒ¨åˆ†çš„channelï¼Œå¯ä»¥è®©é€‰æ‹©opå˜å¾—less biased(æ˜¯convå’Œmaxä¹‹é—´ï¼Œregularizeäº†preference for weight-free operation)
        * å¯ä»¥è®¤ä¸ºè¿™ä¸€æ¡æ˜¯åœ¨å¤„ç†æœç´¢æ—©æœŸæ›´åå‘æ— paramçš„opï¼Œè€Œä¸”ä½œè€…æŒ‡å‡ºè¿™ç§ç°è±¡åœ¨Proxy taskéš¾çš„æ—¶å€™æ›´åŠ æ˜æ˜¾(æ¯”å¦‚imgnet)
    * åŠ£åŠ¿
      * ç”±äºæ¯æ¬¡forward sampleå‡ºæ¥çš„channelåœ¨å˜åŒ–ï¼Œåè€Œä¼šè®©searchå˜å¾—æ›´åŠ unstableï¼Œè§£å†³è¿™ä¸ªé—®é¢˜æå‡ºäº†edge norm
        * (?ç†è§£å¥½åƒè¿˜æ˜¯å·®ç‚¹æ„æ€)
      * ä¸ºæ¯ä¸ªedgeå†å¼•å…¥ä¸€ä¸ªè®­ç»ƒå‚æ•°beta_ij(å¯¹æ¯ä¸ªedgeåšsoftmax),å°†alphaå’Œbetaä¹˜èµ·æ¥ä½œä¸ºåŸæœ¬çš„alpha
        * å¯¹æ¯ä¸ªedgeå¼•å…¥ä¸€ä¸ªnormçš„è¶…å‚æ•°
        * æˆ‘ç†è§£ä¸ºä¸è®©æŸä¸ªedgeå˜å¾—ç‰¹åˆ«significant(?)
        * æœ€åæŠŠalphaå’Œbetaç›¸ä¹˜æ¥å†³å®šop
      * ç”±äºä¸ç®¡é‡‡æ ·å‡ºæ¥çš„channelæ˜¯å•¥ï¼Œbeta_ijæ˜¯æ¯ä¸ªedgeæ‰€ç‹¬æœ‰çš„æ€§è´¨
        * å¯ä»¥è®¤ä¸ºç±»ä¼¼æ˜¯ä¸€ç§attention
  * edge normalization: 
    * ä»¥å¢å¼ºæœç´¢çš„ç¨³å®šæ€§ï¼ˆå‡å°äº†searchçš„uncertaintyï¼‰
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 


#### 7-3. [GOLD-NAS: Gradual, One-Level, Differentiable](http://arxiv.org/abs/2007.03331)
* ğŸ”‘ Key:   
  * é©dartsçš„å‘½ï¼š
    1. dartsçš„bilevel-optå‡è®¾ä¸å¯¹ï¼Œå¼•å…¥é—®é¢˜ï¼Œæ”¹
		* ç†è®ºä¸Šä¼šå¸¦æ¥incorrect gradient estimation
    2. dartsçš„search-space heuristicå¤ªå¤šï¼Œæ”¹
    3. dartsæœ€ç»ˆé™åˆ¶åªèƒ½æœ‰ä¸€ä¸ªopï¼Œæ”¹
* ğŸ“ Source:  
	* Huawei Noah
* ğŸŒ± Motivation: 
	* start from a fully-connected network, then prune out
	* eliminate op with flops constraint instead of heuristic rule
* ğŸ’Š Methodology:
	* å°†softmaxæ”¹æˆäº†elementw-wiseçš„Sigmoidï¼Œä¸ºäº†ä¸äº’æ–¥ï¼Œä»¥æ»¡è¶³æ¯ä¸ªedgeä¸Šä¸ä»…ç•™å­˜ä¸€ä¸ªop
	* ç”±äºä¸æ˜¯å•çº¯æœä¸€ä¸ªcellï¼Œè€Œæ˜¯å„ä¸ªcellç‹¬ç«‹ï¼Œæ‰€ä»¥éœ€è¦æ”¾ä¸‹ä¸€ä¸ª20cellçš„supernet
		* ä¸ºäº†èŠ‚çœæ˜¾å­˜(æˆ‘æ„Ÿè§‰æœ‰ç‚¹ç‰µå¼º)ï¼Œæ‰€ä»¥followäº†å…¶ä»–å‡ ç¯‡æ–‡ç« ï¼Œæ¯ä¸ªedgeåªæœ‰ä¸¤ä¸ªop skip-connectä»¥åŠsep-conv
		* one-level optimization:
			* ç”±äºweightçš„æ•°é‡å¤ªå¤§äº†ï¼Œå¯¼è‡´å¾ˆå®¹æ˜“biasedåˆ°æ‹Ÿåˆweight
			* å½“datasetå¤§çš„æ—¶å€™one-level optå¹¶æ²¡æœ‰é—®é¢˜ï¼Œå°datasetçš„æ—¶å€™ç”¨cutoutä»¥åŠautoaugmentæœ‰æ•ˆ
	* åŸæœ¬çš„æœ€åderiveæ—¶å€™åšdiscretizeçš„æ—¶å€™ç›´æ¥åšhard pruning(é€‰å–æˆ–è€…é‡‡æ ·ä¸€ä¸ªæœ€é«˜å¯èƒ½çš„op),ä¼šé€ æˆå¾ˆå¤§çš„error
		* è¿™ä¸ªæ–¹æ³•ä¸æ’é™¤opï¼Œæ²¡æœ‰softmaxçš„å†…éƒ¨äº’æ–¥ï¼Œæ‰€ä»¥æœ€ä¼˜ç­–ç•¥ä¸€å®šæ˜¯ä¿ç•™æ‰€æœ‰opï¼Œæ‰€ä»¥è¦åŠ regulariazation,ç”¨çš„æ˜¯flops
		* åŒæ—¶å¦‚æœæŸä¸ªopçš„probè¶³å¤Ÿå°ï¼Œè®¾ç½®ä¸€ä¸ªé˜ˆå€¼å°†å®ƒå‰ªæ‰
		* å«åšgradual pruning with resource constraint
		* æ§åˆ¶æƒé‡çš„\lambdaä»0å¼€å§‹ä¸æ–­å¢åŠ 
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 
	* good description of the DARTS search space
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200808095049.png)
	* DARTSçš„ä¸åˆç†çš„åœ°æ–¹
		* æ¯ä¸ªedgeåªèƒ½ä¿æŒä¸€ä¸ªop
		* æ¯ä¸ªinner nodeåªèƒ½ä»å‰é¢è·å¾—ä¸¤ä¸ªè¾“å…¥(è¿˜æ²¡æœ‰åœ¨åŸæ–‡ä¸­æ‰¾åˆ°è¿™ä¸ªç»†èŠ‚ï¼Œä½†æ˜¯çœ‹æœå‡ºæ¥çš„archå›¾é‡Œå¥½åƒæ˜¯æœ‰çš„)
		* æ‰€æœ‰çš„cellæ˜¯å…±äº«çš„ï¼Œlow-levelä»¥åŠhigh-levelçš„ç»“æ„ä¸€è‡´å…ˆç„¶ä¸åˆç†

#### 7-4. [DARTS+: Improved Differentiable Architecture Search with Early Stopping](http://arxiv.org/abs/1909.06035)
* ğŸ”‘ Key:  
  * å‘ç°dartsçš„supernetå¦‚æœè®­ç»ƒè¿‡å¤šçš„epochä¼šcollapseï¼Œå¯¼è‡´æ‰€æœ‰çš„éƒ½æ˜¯shortcut(å­¦å‡ºæ¥çš„æ¶æ„å¾ˆshallowï¼Œlearnable paramå¾ˆå°‘)
  * è®¤ä¸ºcollapseçš„åŸå› æ˜¯cooperate and competitionåœ¨bilevel optå½“ä¸­
    * competitionå­˜åœ¨äºalphaä¸weightä¹‹é—´
  * è§£å†³æ–¹æ³•æ˜¯ç”¨äº†early stoppingï¼šå°±æ˜¯æ¥ç€è®­ç»ƒï¼ŒåŠ äº†ä¸€äº›heuristic ruleï¼Œæ»¡è¶³çš„æ—¶å€™ç»“æŸ(seems like hot fix)
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
  * è¿™ç§æœ¬æ¥åº”è¯¥æ˜¯cooperationå˜æˆcompetitionçš„é—®é¢˜æ˜¯ç”±äºbi-level optimizationæ‰€äº§ç”Ÿçš„
  * GANä¸­ä¹Ÿä¼šå‡ºç°ï¼Œå½“discriminatoråˆ†ç¦»çš„è¶³å¤Ÿå¥½çš„æ—¶å€™ï¼Œgeneratorä¼šé­é‡gradient vanishment
  * è¯´ç™½äº†æ ¸å¿ƒé—®é¢˜è¿˜æ˜¯imbalanced trainingï¼ˆå¯¹äºalphaä»¥åŠweightè€Œè¨€ï¼‰
* ğŸ’Š Methodology:
  * å½“ä¸€ä¸ªcellä¸­å‡ºç°æ›´å¤šçš„skipçš„æ—¶å€™
  * å½“æœ‰è¾ƒå¤šä¸ªepoch alphaé€æ¸å˜å¾—stableäº†
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 

#### 7-5. [Stabilizing DARTS with Amended Gradient Estimation on Architectural Parameters](http://arxiv.org/abs/1910.11831)
* ğŸ”‘ Key: 
  * è®¤ä¸ºåŸå…ˆçš„bilevel optå¯¹alphaæ¢¯åº¦çš„é”™è¯¯æ›´æ–°(1st & 2nd order dartsçš„è¿‘ä¼¼éƒ½æœ‰é—®é¢˜)  
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
  * å‘ç°å¤šè®­ç»ƒsupernetç²¾åº¦æå‡ä¹‹åï¼Œé‡‡æ ·å‡ºæ¥çš„subnet accä¼šä¸‹é™
  * *ä¹Ÿæœ‰åˆ«çš„paperè®¤ä¸ºsupernetè¶Šå¥½subnetå°±è¶Šå¥½ï¼Œåªæ˜¯discreteçš„å½¢å¼ä¸ä¸€æ ·*
* ğŸ’Š Methodology:
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 

#### 7-6. [Fair DARTS: Eliminating Unfair Advantages in Differentiable Architecture Search](https://arxiv.org/abs/1911.12126)
* ğŸ”‘ Key:   
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
* ğŸ’Š Methodology:
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 

#### 7-7. [Efficient Neural Architecture Search via Proximal Iterations](https://arxiv.org/abs/1911.12126)
* ğŸ”‘ Key:   
* ğŸ“ Source:  
* ğŸŒ± Motivation: 
* ğŸ’Š Methodology:
* ğŸ“ Exps:
* ğŸ’¡ Ideas: 


#### 8. [Hierarchical Representations for Efficient Architecture Search](https://shimo.im/sheets/TkdXd9ptKTjDY83R/MODOC)
* ğŸ”‘ Key:     
  * **Hierarchical Search Space** 
* ğŸ“ Source:   
  * ICLR2018 / Google Brain   
* ğŸŒ± Motivation:  
  * æµè¡Œçš„Cell-Basedçš„æ¶æ„ç›¸å¯¹generalï¼Œä½†æ˜¯æœ‰predefinedçš„meta-arch(æ¯”å¦‚è¿™å‡ ä¸ªCellåº”è¯¥æ€ä¹ˆå †å ä¹‹ç±»çš„)ä¸å¤Ÿgeneral
* ğŸ’Š Methodology: 
  * hierarchical genetic representation
    * æ¨¡ä»¿çš„æ˜¯modularized design pattern
  * é‡‡ç”¨äº†EAï¼ŒæŒ‡å‡ºæœ€naiveçš„random searchä¹Ÿå¯ä»¥è·å¾—ä¸é”™çš„æ•ˆæœ
  * flat representation - å°†NNä½œä¸ºä¸€ä¸ªDAG
    * å°çš„graph motifç»„æˆä¸€ä¸ªå¤§çš„graph motif
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308204233.png)
  * Tournament Selection Search
* ğŸ“ Exps:       
* ğŸ’¡ Ideas: 

#### 9. [PNAS - Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)
* ğŸ”‘ Key:      
  * **Progressive** -> Easy2Hard
* ğŸ“ Source:
  * ECCV 2018 / Google AI      
* ğŸŒ± Motivation:  
  * Progressive (Simple 2 Complex)
* ğŸ’Š Methodology:
  * Cell-based Search Space 
  * NO RL or EA, Sequential-Model-Based Method
  * æ¯ä¸€æ­¥åšä¸€ä¸ªå±€éƒ¨çš„Heuristic Searchï¼Œä»¥å‰ä¸€æ­¥çš„Predictoræ¥é€‰å–ä¸‹ä¸€æ¬¡çš„Predictor
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:   



#### 7-A. [SNAS-Stochastic NAS](https://arxiv.org/pdf/1812.09926.pdf)

* ğŸ”‘ Key: 
  * **Gumble Softmax** in DARTS    
* ğŸ“ Source:      
  * ICLR2019 / SenseTime
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  



#### 10. [NAO-Neural Architecture Optimization](https://arxiv.org/abs/1808.07233)

* ğŸ”‘ Key: 
  * Arch **Encoder-Decoder**   
* ğŸ“ Source:      
  * NIPS2018 / MSRA
* ğŸŒ± Motivation:  
  * Predictor-based
  * æ¶‰åŠäº†ä¸€ä¸ªArchçš„Encoderï¼Œå°†archæ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´ï¼Œåœ¨è¯¥ç©ºé—´ç”¨Gradientä¼˜åŒ–
* ğŸ’Š Methodology:
    * Discrete -> Continuos 
      * åŒ…å«äº†ä¸€ä¸ªencoderï¼Œå°†archæ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´ï¼ŒåŒæ—¶è¿˜æ­é…ä¸€ä¸ªdecoder
      * è¿˜æœ‰predictor 
    * ä¸æ­¤ç±»ä¼¼çš„æ˜¯DARTSï¼Œè¯´DARTSè®¤ä¸ºæœ€å¥½çš„archæ˜¯å½“å‰weightä¸‹çš„argmaxï¼Œè€ŒNAOç›´æ¥ç”¨ä¸€ä¸ªdecoderæ˜ å°„å›æ¨¡å‹
      * è¿˜æœ‰ä¸€æ”¯æ˜¯Bayesian Optimizationï¼Œä½œè€…è®¤ä¸ºGPçš„æ€§èƒ½äºCovariance Functionçš„è®¾è®¡å¼ºç›¸å…³
    * Search Spaceè®¾è®¡
      * ä¸¤æ­¥ï¼Œé¦–å…ˆå†³å®š1ï¼‰which 2 previous nodes as inputs 2)ç¡®å®šè¦ç”¨ä»€ä¹ˆop
    * Encoderå’ŒDecoderéƒ½æ˜¯LSTMï¼Œpredictoræ˜¯ä¸€ä¸ªmean-poolingåŠ mlp
    * ä¸‰è€…Jointly Train
      * è®¤ä¸ºpredictor could work as regularizationå»é¿å…encoderåªå¯¹åº”decoderçš„ç»“æœï¼Œè€Œæ²¡æœ‰æ­£å¸¸è¡¨å¾
        * è¿™ä¸€æ­¥å’Œä¼ ç»ŸVAEä¸­çš„åŠ noiseä¸€è‡´
    * è®¤ä¸ºweight-sharingå’ŒNAOæ˜¯complementaryçš„
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  
    * symmetricçš„design:ä¸ºäº†ä¿è¯symmetricçš„æ¨¡å‹ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„embeddingä¸€è‡´ï¼Œpredictorç»™å‡ºå·®ä¸å¤šçš„ç»“æœ
      * ç”¨äº†Augmentationï¼ˆflipï¼‰æ¥è®­ç»ƒencoder


#### 11. [ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware](https://arxiv.org/abs/1812.00332)
* ğŸ”‘ Key:  
  * è§£å†³Gradient-basedçš„æ–¹æ³•æœ‰å¤§é‡Memory Consumptionï¼Œè¿›è€Œéœ€è¦ProxyTaskçš„ååŠ©
  * multi-binary mask choice as path-level pruning       
* ğŸ“ Source:      
  * Han MIT
* ğŸŒ± Motivation: 
  * Gradient-basedæ–¹æ³•ç½‘ç»œæ˜¾å­˜å ç”¨å¤§ï¼Œå¯¹å¤§ä»»åŠ¡æ¥è¯´å›°éš¾ã€‚
* ğŸ’Š Methodology: 
  * è§£å†³Gradient-basedçš„æ–¹æ³•æœ‰å¤§é‡Memory Consumptionï¼Œè¿›è€Œéœ€è¦ProxyTaskçš„ååŠ©    
  * Path-level binarization(path level pruning)ä»¥å‡å°‘ç°å­˜æ¶ˆè€—
  * Latency regularization loss
  * Network representation
    * n edges - e_i
    * N candidate primitive operations o_i
    * each edge to be a mixed operation has N parallel paths
    * given input x , output is outputs of N paths
      * in One-shot means Sum
      * in Darts means Weighted Sum
        * weights is softmax of \alpha_i (N alphas)
      * roughly needs N times GPU memory
      * in this paper
        * binray mask with probability of softmax weight in DARTS
        * think the probability is StraightThrough
  * training
    * when training arch, freeze weight
    * when training weight, only trains weights on active path
  * How 2 actual solve the memory issue
    * factorizing the task into choosing 1 path out of N candidates into multiple binary mask
    * sample 2, only 2 paths are involved 
    * use grad of these 2 path to update the arch weight of this 2 path
    * because all path's arch weights needs softmax, so need a ratio for 2 paths, to keep the unsampled paths weight unchanged
    * one path weight enhanced, the other attenuated
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:     
  * like One-shot and DARTS
    * no need for meta-controller in shared-weights
    * model NAS as a simple training of an over-parameterized network
      * One-shot with DropPath
    * Pruning - Path-level pruning for NAS  

---


#### [Overcoming Multi-Model Forgetting in One-Shot NAS with DiversityMaximization](https://shiruipan.github.io/publication/cvpr-2020-zhang/)
* ğŸ”‘ Key:
  * è§£å†³ä¼ ç»Ÿçš„Shared-Weightsæ–¹æ³•åœ¨ä¼˜åŒ–æ–°çš„æ¶æ„çš„æ—¶å€™è€çš„æ¶æ„ç²¾åº¦ä¼šä¸‹é™(Catastrophic Forgetting)çš„é—®é¢˜
* ğŸ“ Sourceï¼š
  * CVPR 2020
* ğŸŒ± Motivation: 
    * ä¼ ç»Ÿçš„OneShotæ–¹å¼è®¤ä¸ºJointly Optimized Supernet Weightsæ˜¯æœ€ä¼˜çš„
    * ä½†æ˜¯sequentially train archs with partially-shared weightsä¼šå¯¼è‡´Catastrophic Forgetting
    * æ–‡ç« æ ¸å¿ƒæŠŠOne-ShotNASçœ‹æˆä¸€ä¸ªContinual Learningçš„é—®é¢˜(Constrained Optimzation,learning of current arch should not degrade previous much)
* ğŸ’Š Methodology:
    * NSAS(Search-based Architecture Selection) Loss Function 
    * Enforce the architectures inheriting weights from the supernet in current step perform better than last step
    * å¦‚æœç´¯è®¡çš„è¯è¦æ±‚ä¼šå¤ªé«˜äº†ï¼Œæ‰€ä»¥ä¸æ˜¯é™å®šå…¨éƒ¨çš„Previous Archï¼Œè€Œæ˜¯é€‰æ‹©å…¶ä¸­çš„ä¸€ä¸ªSubset(å¯¹äºå¦‚ä½•é€‰å®šè¿™ä¸ªSubsetæ˜¯å‡å®šSubsetä¸­çš„Archè¦æœ‰Diversity-æ‰¾åˆ°æœ€å¤§Diversityçš„è¿‡ç¨‹å°±æ˜¯æ‰€ä¸ºçš„Novelty Search)
    * å®ç°çº¦æŸçš„æ–¹å¼æ˜¯åŠ ä¸€ä¸ªSoft Regularization


#### [One-Shot Neural Architecture Search via Self-Evaluated Template Network](https://arxiv.org/abs/1910.05733)
* ğŸ”‘ Key:
  * ä¼ ç»Ÿçš„Evluationæ…¢ï¼ŒShared Weightsçš„æ–¹å¼é€‰å–å»Evaluateçš„ç»„ä»¶çš„æ—¶å€™æ˜¯Randomçš„ï¼Œä¸å¤ŸInstructive
* ğŸ“ Sourceï¼š
  * ICCV 2019 / Xuanyi Dong, Yi Yang
* ğŸŒ± Motivation: 
  * æå‡ºäº†ä¸€ä¸ªSETN(Self Evaluated Template Network)
    * ä¸€ä¸ªEvaluatorå»é¢„æµ‹æœ‰æ›´ä½Valid Lossçš„æ¶æ„(ç±»ä¼¼ä¸€ä¸ªPredictor)
    * ä¸€ä¸ªæ¨¡æ¿Templateç½‘ç»œå»Shared Params,åŒ…å«äº†æ‰€æœ‰çš„Candidate
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402210522.png)
    * çœ‹ä¸Šå»å°±æ˜¯å¯¹Shared-weightsåŠ äº†ä¸€ä¸ªPredictorä½œä¸ºControllerï¼Œå»ä»ä¸€ä¸ªæ‰€è°“çš„Templateç½‘ç»œä¸­é‡‡æ ·å‡ºå­æ¶æ„ï¼ŒControlleræ¥å†³å®šæ€ä¹ˆé‡‡(è€Œä¸æ˜¯éšæœºé‡‡æ ·)
  * Nä¸ªCell
    * æ¯ä¸ªCellä¸­Bä¸ªBlock
    * æ¯ä¸ªBlockå¯èƒ½æ˜¯4å…ƒç»„
  * Candidate Network 
    * Contain All candidate CNN in search space
    * train stochsticly - uniformly sample 1 candidate and only optimize its params
      * Optimize each with equal possibility
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402212418.png)
      * Iæ˜¯inputæ˜¯çº¯éšæœºSampleï¼ŒFæ˜¯Functionï¼Œå…¶ä¸­çš„orderæŒ‡çš„æ˜¯å†ä¸€ä¸ªé›†åˆOä¸­é‡‡æ ·ï¼Œå…¶ä¸­f1çš„indexä¸€å®šè¦å°äºf2
    * Evaluatorï¼š
      * Encode one CNN candidate as a set of quadruples
      * ä»categorical distribution sampleå‡ºä¸€ä¸ªchoiceï¼Œç”¨softmax normalized valueä½œä¸ºvectorå€¼
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402213049.png)
* ğŸ’¡ Ideas:
    * æŒ‡å‡ºæœ‰äº›Shared-Weightsçš„æ–¹æ³•ä¼šshared parameter with a learnable distribution of archs
      * ä½œè€…è®¤ä¸ºè¿™æ ·ä¼šæœ‰biasï¼Œå› ä¸ºç›¸å¯¹lightweightçš„modelä¼šæ›´å¿«æ”¶æ•›ï¼Œlearnable distribution will bias to these model
      * â€œthe Matthew effectâ€ to refer that some quickly-converged candidates will get more chances  to be further optimized in some NAS algorithms


#### [GATES]()
* ğŸ”‘ Key: 
  * NASä¸­çš„**Predictor**é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªæ›´å¥½çš„Encoder
* ğŸ“ Source
  * Arxiv & THU EE
* ğŸŒ± Motivation: 
  * Current Encoder model topological information implicitly
  * åŸæœ¬çš„gcnä¹‹ç±»çš„encoderæ–¹å¼edges stand for notion of affinity, feature on node
  * View NN as Data-Processing Graph
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200404200848.png)
  * è®¤ä¸ºä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿç›´æ¥Handle Topological isomorphism,ä»¥åŠå¯¹Information Processingçš„å»ºæ¨¡
* ğŸ“ Exps: 
  * Nb101/201ä¸ŠKDï¼ŒNatKï¼ŒPatK
* ğŸ’¡ Ideas:
  * ç›¸å…³æ–‡ç« NAOï¼Œç”¨ä¸€ä¸ªEncoder-Decoderæ¥åš
  * In the Kendall's Tau measure, all discordant pairs are treated equally
  * å…¶ä»–æŒ‡æ ‡(å› ä¸ºkendall tauå…¶å®è€ƒè™‘äº†å¾ˆå¤špoor archçš„ç›¸å¯¹å…³ç³»ï¼Œå¯¹äºNASæ¥è¯´ç”¨å¤„ä¸å¤§)
    * NatKï¼š predictç»™å‡ºçš„Kä¸ªä¸­æœ€å¥½çš„å®é™…rank
    * PatKï¼š Predictor TopKåœ¨Gt TopKä¸­çš„æ¯”ä¾‹
  * Nb101 - 432k archs - Op on Node
  * Nb201 - 15625 - OP on Edge


---

### [How to Train Your Super-Net: An Analysis of Training Heuristics in Weight-Sharing NAS](https://arxiv.org/abs/2003.04276)
* ğŸ”‘ Key:         
  * Analysis of One-Shot NAS SuperNet Training
* ğŸ“ Source:      
  * Arxiv 2003
* ğŸŒ± Motivation:  æ•…äº‹
  * å¯¹äºSuperNetè®­ç»ƒçš„ä¸€ä¸ªAblation
  * éœ€è¦å¥½å¥½è°ƒå‚ï¼Œä¸åº”è¯¥ç”¨å°½é‡å°‘çš„epochè€Œåº”è¯¥ç”¨Subset
* ğŸ’Š Methodology: æ–¹æ³•
  * ç”¨FairNASä½œä¸ºå¹³å°
  * **ä¸»è¦é‡ç‚¹åœ¨äºè®­ç»ƒå‚æ•°**(æ˜¯å¦æœ‰Affineï¼ŒLRï¼ŒWD)
  * è¯„ä»·æŒ‡æ ‡æ˜¯è¶…ç½‘ç»œçš„å‡†ç¡®ç‡ï¼Œè¶…è¿‡éšæœºæœç´¢çš„æ¦‚ç‡,è¿˜æœ‰kendall-tauã€‚
  * æœ€åä¸€ä¸ªæ˜¯ä»è¶…ç½‘ç»œéšæœºé‡‡æ ·200ä¸ªæŒ‘3ä¸ªæœ€å¥½çš„ï¼Œå–ä»–ä»¬çš„ground-truthçš„å¹³å‡
* ğŸ“ Exps:        å®éªŒ
* ğŸ’¡ Ideas:       æƒ³æ³•


### [DetNAS: Backbone Search for Object Detection](http://arxiv.org/abs/1903.10979)
* ğŸ”‘ Key:   
  * nas 4 Det backbone
  * Supernet
* ğŸ“ Source:  
  * Megvii
* ğŸŒ± Motivation: 
  * Det often needs imagenet pretraining & NAS requires accuracy as supervised signal
    * Imagenet-pretraining + Det finetune
  * Pre-training and finetuning are costly
    * Following One-shot, decouple the weight training and the architecture
  * Det task perf. as guide, to search for the Backbone Feature Extractor
* ğŸ’Š Methodology:
    * Steps: 
      1. SuperNet Pretrain on ImageNet and finetune on Det Task
      2. NAS on Supernet with EA
        * Path-wise(at one time, only updating samples path)
    * Finetuning BN
      * Freezing BN(Traditional) wont work for supernet(normalize couldnt acquired at different paths)
      * SyncBN replace regular BN
        * Compute BN Statistics across multiple GPUs (save memory consumption)
      * Also when EA searches arch, each BN param should be independent
        * need to re-accumulate the BN for every new arch
    * SS Design
      * Small/Big (40/20 Blocks)
        * Small used in Ablation Study
      * based on ShuffleNetV2 Block(involves channel split and shuffle operation)
        * 4 Choices:
        * x3: kernel-size [3,5,7]
        * x1: replacing right branch with Xception block(3 repeated DW 3x3 Conv)
        * 4^(40) choices for big ss
    * EA
      * Mutation + CrossOver
      * arch dont meet constraint will be removed when updating
* ğŸ“ Exps: 
* ğŸ’¡ Ideas:
  * Det's Direction
    * Architecture: FPN - Top-Down arch with lateral connection, integrating features at all scales
    * Loss: RetinaNet's Focal Loss, dealing with the class imbalance(Instability in earlier training?)
    * MetaAnchor: dynamic anchor mechanism
  * AmeobaNet - plain EA without Controller could also achieve
  * Det often uses the image classification backbone which could be sub-optimal(DetNet59 > ResNet101)
  * NAS-FPN only searches for the Feature Pyramid Network
  * EA could better handle Constraints than RL & Gradient-based 
  * [SyncBN](https://tramac.github.io/2019/04/08/SyncBN/)
    * Plain BN with DataParallel - Input distiributed to subsets and build different models on different GPU
    * (Since independent for each GPU, so batch size actually smallen)
    * Key2SyncBN: Get the global mean & var
      * Sync 1st then calc the mean & var
      * (Simplest implemention is first sync together calc mean, then send it back to calc var) - Sync twice
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418111031.png)
        * modify computation flow and only sync once
    * [Code](https://github.com/tamakoji/pytorch-syncbn)
  * ------------------------------------------
  * Rather Small SS
  * Simple Shared-Weights with interesting handle of BN


### [NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection](http://arxiv.org/abs/1904.07392)
* ğŸ”‘ Key:   
  * Search a FPN(Feature Pyramid Network) in a ss covering all cross-scale connections
* ğŸ“ Source:
  * Quoc V Le Google  
* ğŸŒ± Motivation: 
  * Huge design space(increase exponentially)
* ğŸ’Š Methodology:
  * Following Cell-based SS(author called it as scalable architecture), main contribution is designing **search space**
    * the SS is **modular**  (Repeat the FPN N times then concat into a large net)
  * RNN RL Controller
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418171859.png)
  * Feature Pyramid Network
    * (following RetinaNet, use last layer in each group of feature map as *the input of the FPN*)
    * 5 Scales(C_{3,4,5,6,7}) stride of 8/16/32/64/126 pixelï¼ˆ6,7 purely max-pooling of 5ï¼‰ for Merging Cell
    * Composed of multiple merging cells  
    * Input/Output same size - can be stacked (num to stack would control the acc/flops trade-off)
    * Each merging cell gives output are appended into the candidate layers, also feeds into next merging cell
      * Finally the 5 merging cells are output 
  * Merging Cell
    * basic element of FPN
    * Merging 2 input feature map of different size
    * Each Cell has its own resolution(the output)
    * 4 Step: 
      1. choose input-layer-1 
      2. choose input-layer-2 
      3. choose output feature resolution
      4. select binary op(add or maxpool) - (scales are handles b4 the binary op)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173400.png)
  * Meshgrid Representation
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418173458.png)
* ğŸ“ Exps:
  * Scalable: Stacking NAS-FPN blocks could improve acc while stacking simple block couldnt
* ğŸ’¡ Ideas:
  * FPN - fuse features across different scales (cross-scale connection in ConvNets)
    * Deeper layer feature semantically strong but less resolution, are upsampled and added with lower-representation for feature with both good semantic and resolution  
    * sequentially combining 2 adjacent layer feature (with top-down/lateral connection)
    * RW
      1. [Path aggregationnetwork for instance segmentation. In CVPR, 2018.]() - add an additional bottom-up pathway
      2. [M2det: A single-shot object detector based on multi-level feature pyramid network. AAAI, 2019.]() - Multiple U-shaped Modules
      3. [Deep feature pyramid reconfiguration for object detection. In ECCV, 2018]() - Combine features at all scale + Global attention
    * Problem: manually designed and shallow(compared to backbone)
  * Any-time detection: dont necessarily need to forward all pyramid networks
    * Desired when computation effort is concern
  * ----------------------------------------------------------------------------------
  * Google's Work, really strange Hyper-paramï¼ˆlr-0.08/8 epochs trainingï¼‰ (Maybe Grid-Search Again?)


### [EfficientDet: Scalable and Efficient Object Detection](http://arxiv.org/abs/1911.09070)
* ğŸ”‘ Key:   
	* Systematically NAS for Det Task
  * Combining EfficientNet Backbone + Bi-FPN + Compound Scaling
* ğŸ“ Source:  
	* Quo V Le Google Brain
* ğŸŒ± Motivation: 
	* Weighted Bi-directional FPN - for multi-scale feature fusion (Better Feature Aggregation)
	* Compound Scaling method - uniformly scale the resolution/depth/width (Scalable)
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418223126.png)
  * the "BiFPN", analogy of FPN & PANet
    * from traditional "Top-down" structure(1-way information flow)
    * Adding an extra edge when I/O is at the same level
    * remove node with only one input edge 
    * Adding Weighted Feature Fusion - like Attention
    * exponential scale up BiFPN width
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200418220047.png)
* ğŸ“ Exps:
* ğŸ’¡ Ideas:
  * One-Stage Det: (Anchor-Free) whether have a region proposal step

### [EcoNAS: Finding Proxies for Economical Neural Architecture Search](https://arxiv.org/abs/2001.01233)
* ğŸ”‘ Key:     
  * Evaluate different proxy    
* ğŸ“ Source:      
  * SDU & NanyangTU & SenseTime
* ğŸŒ± Motivation:  
  * Finding a more stable proxy 
* ğŸ’Š Methodology: 
  * Grid searching different proxy
    1. Channel num
      * Smaller channel means even better(actually coordinates with less epochs)
    2. Input resolution
      * Pretty OK
    3. Epochs
      * Increase will be better, but with a bound
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200427111503.png)
    4. Dataset Size
      * with smaller size, upper bound lower
    5. Network depth
      * DONT do that
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200427111258.png)
    * NASNet/AmoebaNet - bad proxy - (- network depth)
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  


### [Improved one-shot NAS by suppressing posteriror fading](http://xxx.itp.ac.cn/pdf/1910.02543v1)
* ğŸ”‘ Key:  
  * Bayesian in Shared-Weights
  * Posterior Convergent NAS   
  * Bayesian to Solve posterior fading   
    * Guide the parameter-posterior towards true dist  
* ğŸ“ Source:      
  * SDU / Brown / SenseTime
* ğŸŒ± Motivation:  
  * Weight-Sharing: model perform better with shared weights doesnot necessarily better than trained alone
* ğŸ’Š Methodology: 
  * Formulate NAS in bayesian manner
    * and prove that increasing number of arch will push p_shared away from p_alone, called **Posterior Fading**
  * Mitigate PF
    * training with partial model pool
      * gradually shrink the ss
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200427114553.png)
    * evaluation under latency constraint
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:  

### [Searching for Accurate Binary Neural Architectures](http://arxiv.org/abs/1909.07378)
* Huawei Noah - ICCV19 W
* WRPN uniform expand
* only search for width(channels), acquire higher acc with less flops
    * encode channel num into ss, EA as optimization
* the arch remain the same with the original fp32 model
* DoReFaNet Forward
    * é™¤äº†ç¬¬ä¸€å±‚å’Œæœ€åä¸€å±‚
* 4 is empirical upper bound of expansion ratio
    * [0.25,0.5,1,2,3,4]
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094001.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094100.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627094241.png)


### [Learning Architectures for Binary Networks](http://arxiv.org/abs/2002.06963)
* GIST(South Korea)
* seems like eccv ...
* å·ç§°è‡ªå·±å¯ä»¥å’ŒSOTAçš„æ–¹æ³•æ‰“å¹³ï¼Œè€Œä¸ç”¨å¾ˆå¤šæŠ€å·§ï¼Œåªæ˜¯åŠ å¤§
* cell-based, proposed a new cell template composed of binary operations
* é¦–å…ˆå®éªŒç›´æ¥å¯¹Dartsç­‰æœå‡ºæ¥çš„ç»“æ„ç›´æ¥ç”¨XNORçš„binary scheme
    * æ•ˆæœå¾ˆå·®(å¾ˆåˆç†)
* novel-searching objective - Diversity Regularization  
* SS design
    * should be robust to quantization error
    * dialted convä¸ä¸€åŠconvå¯¹Q-erroræ¥è¯´ä¸€è‡´ï¼Œè¿™ä¸¤è€…ç›¸å¯¹å¯¹Q-erroræ¯”è¾ƒé²æ£’
    * separableæœ‰å¾ˆå¤§çš„Q-error
    * zeorise - è¾“å‡ºä¸º0ï¼ŒåŸå…ˆæ˜¯ç”¨æ¥å»ºæ¨¡æ²¡æœ‰shortcut connectionçš„è¿‡ç¨‹
        * æœ¬è´¨æ˜¯å› ä¸ºæœ‰æ—¶å€™binaryä¹‹åçš„è¯¯å·®å®åœ¨æ˜¯å¤ªå¤§äº†ï¼Œå¯¼è‡´æ¯”ç›´æ¥æŠŠç»“æœç½®0è¿˜å¤§
        * ä¿ç•™è¿™ç§å±‚å»å‡å°‘Q-Errorï¼Œè€Œä¸æ˜¯åªæ˜¯å°†å…¶ä½œä¸ºPlaceholder(?)
        * æœ‰ä¸€ä¸ªpossibilityæ˜¯å¦åŒ…å«zeroise
* Cell Template Deisgn
    * unstable gradient
    * å¼ºåˆ¶ä¸åŒCellä¹‹é—´å¸¦Skip - InterCell Skip connection(less quantization error)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629131618.png)
* Diversity Regularizer
    * åŒåˆ«çš„æ–‡ç« ï¼Œä¸€æ ·å‘ç°äº†å¸¦å‚æ•°çš„opä¸€å¼€å§‹ä¸å®¹æ˜“è¢«é€‰ä¸­
    * exponential annealed entropy regularizer

* çœ‹ä¸Šå»åƒæ˜¯ä¸€ä¸ªå¸¦Hotfixçš„æ–¹æ³•ï¼Œä½†æ˜¯åšçš„è¿˜æ˜¯æ¯”è¾ƒsolidçš„


### [Binarized Neural Architecture Search](http://arxiv.org/abs/1911.10862)
* Beihang Univ
* Darts foundation
* channel sampling / operation space reduction0
   * abbadon less potential operation
* åŸºæœ¬å°±æ˜¯PCDarts+Binaryå¤è¿°äº†ä¸€ä¸‹â€¦

### [CP-NAS: Child-Parent Neural Architecture Search for Binary Neural Networks](http://arxiv.org/abs/2005.00057)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629140108.png)
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200629140956.png)
    * sample without replacement 
    * å¯¹Kä¸ªopæ¯ä¸ªsampleè¿‡ä¸€æ¬¡ï¼Œï¼Œå¾ªç¯Kæ¬¡ï¼Œä¹‹ååšSS reduction
* Pair Optimization for binary
    * minimize distribution error between fp32 and binary
    * minimize output-class inrta-class feature 

* [BATS: Binary ArchitecTure Search](http://arxiv.org/abs/2003.01711)
* Cambridge
* è¡¨ç¤ºç›´æ¥æŠŠNASå¥—ç”¨åˆ°binary domainä¼šå¸¦æ¥å¾ˆå¤§é—®é¢˜ï¼Œæ‰€ä»¥éœ€è¦ä¸€äº›æ“ä½œå»alleviate
    * binarized ss
    * search strategy (control and stablilze the searching)
        * temperature-based
* binaryçš„æ–¹å¼
    * follow XNORNet - ä½†æ˜¯scaling factor åä¼ å¾—åˆ°è€Œä¸æ˜¯analytically
* search space
    * é¦–å…ˆè¡¨ç¤ºä¸€ä¸ªæ¯”è¾ƒå¥½çš„sså³ä½¿ç”¨random searchä¹Ÿå¯ä»¥è·å¾—æ¯”è¾ƒå¥½çš„æ•ˆæœ
    * è®¤ä¸ºdepthwiseæœ¬èº«å·²ç»æ˜¯compactäº†ï¼Œæ‰€ä»¥æ›´éš¾åšbinary - bottleneckä¹Ÿæ˜¯
        * high group size å»è¿‘ä¼¼ depthwise
        * æœ‰å°è¯•è¿‡åœ¨æ¯ä¸ªgroupä¹‹ååŠ å…¥ä¸€ä¸ªchannel shuffle,ä½†æ˜¯å…¶å®æ²¡æœ‰å¤ªå¥½çš„æ•ˆæœï¼ŒçŠ¹è±«groupä¸€èˆ¬æ¯”è¾ƒå¤§
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627095554.png)
* Search Strategy - å¯¹DARTSçš„ç¨³å®šæ”¶æ•›çš„å„ç§æ“ä½œ
    * æ—©æœŸå‘ç°ä¼šå¾ˆå¿«æ”¶æ•›åˆ°real-value op(æ¯”å¦‚polling and skip-connect),æ—©æœŸæ¯”è¾ƒæœ‰æ•ˆ
    * ç”¨temerpatureæ¥è§£å†³ï¼Œè®©æ•´ä¸ªåˆ†å¸ƒå˜å¾—æ›´åŠ spiky
* 2-Stage Search ç”±äºTraining Binaryæœ¬èº«æ›´å›°éš¾ä¸€ç‚¹
    * weights realï¼Œ activation binarized
    * ä¸ªäººæ„Ÿè§‰è¿™ä¸ªä¸å¤§é è°±â€¦ä¹Ÿä¸ä¸€å®šâ€¦
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200627100500.png)

### [APQ: Joint Search for Network Architecture, Pruning and Quantization Policy](http://arxiv.org/abs/2006.08509)
* MIT Han
* å°†Archï¼ŒPrunä»¥åŠQuantize unififyåˆ°ä¸€ä¸ªæ–¹å‘
* è¶…å¤§SS,ç”¨ä¸€ä¸ªQuantize Predictor
    * è®­ç»ƒå…¶éœ€è¦ä¸€ä¸ª{FP,QUAN}çš„Acc Pair,éœ€è¦è®¾è®¡Quantize-aware finetune
    * å€ŸåŠ©Transefer Knowledge(ä»FP32 predictoråˆ°Quant predictor),æ˜¾è‘—æå‡Sample Eff
* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200630235848.png)
    * ä¸»ä½“æ˜¯Evo+Predictor+Shared-Weights(OFA)
* OFA Training - progressively distill smaller subnets sampled from the OFA
    * MobileNet V2 Base
    * to handle SS è¿‡å¤§çš„æ—¶å€™OFAçš„subnetä¸å‡†
* Quantization Predictor
    * Arch and Quantize Policy encoding


### [Binarizing MobileNet via Evolution-based Searching](http://arxiv.org/abs/2005.06305)

* ğŸ”‘ Key:         
	1. find a balanced bianry mobilenet, mainly in the group-conv domain
	2. weight sharing 
* ğŸ“ Source:      
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
	* BinaryScheme
		* scaling factor and backprop like XNORNeto
		* enhanced shortcut like MoBiNet & Birealnet
		* Polynominal differentiable approximation like birealnet
		* only weighs are binarized at train/testo
	* Flow
		1. pre-training
		2. sample grouping strategy and EA 
		3. determine the strategy and train from scratch
	* module modification
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200724163159.png)
		* ![](https://github.com/A-suozhang/MyPicBed/raw/master//img/20200724163208.png)
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:       
	* depth-wise + point-wise = depth separable conv
		* for binary the channel(depth)wise, less binary numbers are added together and has low precision, so cannot converge
		* so group conv could be a surrogate





## Reference

* [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)
* [AutoML.org](https://www.automl.org/automl/literature-on-neural-architecture-search/)

 
