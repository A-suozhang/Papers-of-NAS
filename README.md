# Awesome-Neural-Architecture-Search

> ä¾¿äºä¸ªäººæŸ¥æ‰¾çš„ä¸€äº›NASæ–‡ç« çš„æ¢³ç†ä»¥åŠç®€è¦Digest
> é‡‡ç”¨äº†ä¸[Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)ä¸åŒçš„é€æ¨¡å—   æ¢³ç†æ–¹å¼ï¼Œä¾¿äºä¸ªäººç†è§£ä¸é€ŸæŸ¥

## Genre


## Paper Digest


```
-----------------------------------
* Format
* ğŸ”‘ Key:         æ ¸å¿ƒ
* ğŸ“ Source:      æ¥æº
* ğŸŒ± Motivation:  æ•…äº‹
* ğŸ’Š Methodology: æ–¹æ³•
* ğŸ“ Exps:        å®éªŒ
* ğŸ’¡ Ideas:       æƒ³æ³•
-----------------------------------
```


* ğŸ”‘ Key:         
* ğŸ“ Source:      
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:       


#### 1. [NAS with RL](https://arxiv.org/pdf/1611.01578.pdf) - Google Brain
* ğŸ”‘ Key:
  * NAS Work Flow
* ğŸ“ Sourceï¼šICLR2017 / Google Brain
* ğŸŒ± Motivation:
  * é¢†åŸŸå¼€ç«¯ï¼ŒNASçš„å¼€å±±,å¥ å®šäº†å¤§è‡´Workflow
  * å»ºç«‹åœ¨è®¤ä¸ºNNçš„connectivityå’Œstructureå¯ä»¥è¢«è¡¨è¾¾æˆä¸€ä¸ªvariable-length stringï¼ˆå¯ç”¨RNNåšembeddingï¼Œä¹Ÿå°±æ˜¯ä½œä¸ºcontrollerï¼‰
    * å°†é‡‡æ ·ï¼ˆRNNç”Ÿæˆï¼‰å‡ºæ¥çš„å­æ¶æ„è¿›è¡Œè®­ç»ƒï¼Œå°†Accä½œä¸ºrewardä¿¡å·ï¼Œé€šè¿‡Policy Gradientçš„æ–¹å¼å›ä¼ å»update controllerï¼ˆOriginal RNNï¼‰
* ğŸ’Š Methodology: 
  * search space
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203201724.png)
  * RNN
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203202126.png)
    * å°†RNNçœ‹ä½œä¸€ä¸ªæ ‘çš„ç»“æ„
* ğŸ“ Exps: 
  * æ¯”cifar10ä¸Šçš„baselineæäº†0.1ä¸ªç‚¹ï¼Œå¹¶ä¸”å¿«äº†ä¸€ä¸¢ä¸¢1.05ï¼Œä»¥åŠå…¶ä»–çš„dataset-PennTreebankä»¥åŠlanguage modelingçš„task
* ğŸ’¡ Ideas:
  * åœ¨relatedworkä¸­è®²åˆ°
    * hyper-param optimizationåªèƒ½å¤Ÿåšåˆ°åœ¨fixed-lengthçš„spaceè¿›è¡Œæ¨¡å‹ä¼˜åŒ–ï¼Œä¸”å¯¹good initial modelæ¯”è¾ƒä¾èµ–
    * Bayesianæ–¹æ³•å¯ä»¥å¯»æ‰¾ä¸€ä¸ªâ€œä¸å®šé•¿â€çš„æ¶æ„ï¼Œä½†æ˜¯ä¸æ˜¯å¾ˆæœ‰æ„ä¹‰
  * The controller in Neural Architecture Search is auto-regressive, which means it predicts hyperparameters one a time, conditioned on previous predictions. This idea is borrowed from the decoder in end-to-end sequence to sequence learning. 


#### 2. [Accelerating NAS using performance prediction](https://arxiv.org/pdf/1705.10823.pdf)

* ğŸ”‘ Key: 
  * **Predictor**        
* ğŸ“ Source: ICLR2018W / CMU   
* ğŸŒ± Motivation:  
  * **æå‡ºpredictoræ¥åŠ é€Ÿevaluation**,ä»¥åŠEarlyStopping
  * è®¤ä¸ºhumanæ˜¯ä»training curveæ¥è§‚å¯Ÿçš„ï¼Œæœ¬æ–‡parameterizeäº†è¿™ä¸ªè¿‡ç¨‹ï¼Œè®­ç»ƒregression modelæ¥é¢„æµ‹acc
  * å¯¹åº”çš„counterpartæ˜¯Bayesiançš„ä¸€äº›æ–¹æ³•
    * ç›®æµ‹æ˜¯äººå·¥è®¾è®¡ä¸€äº›æ‹Ÿåˆlearning curveçš„base functionï¼Œç„¶åç”¨expensiveçš„MCMCæ¥æ‹Ÿåˆ
    * ä¹Ÿæœ‰ç”¨é«˜æ–¯è¿‡ç¨‹æ ¸å‡½æ•°çš„
* ğŸ’Š Methodology: 
  * è¯´ç™½äº†å»ºæ¨¡çš„æ˜¯Training Curve
    * è¾“å‡ºval accï¼Œè¾“å…¥æ˜¯configurationï¼Œåœ¨æ¯ä¸ªtime step
    * åšäº†ä¸€ä¸ªSRMï¼ˆSequential Regression modelï¼‰æ¯”å¦‚RBMï¼ŒRandomForestç­‰
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200203204919.png)
* ğŸ“ Exps:        
* ğŸ’¡ Ideas:      



#### 3. [eNAS - Efficient Architecture Search by Network Transformation](https://arxiv.org/pdf/1707.04873.pdf)

* ğŸ”‘ Key:  
  * **Weight Manager** -> Shared Weights
* ğŸ“ Source:  AAAI2018 / SJTU
* ğŸŒ± Motivation:  
  * **Reusing Weight(Weight Manager)ä¸éœ€è¦from scratchæ¥è®­ç»ƒç½‘ç»œ**
  * metacontrolleré‡‡æ ·æ¶æ„based on Network transformation 
* ğŸ’Š Methodology: 
    * ä¾ç„¶æ˜¯Rl Controller - Policy Gradientæ›´æ–°ï¼Œå…¶actionæ˜¯æŒ‡å¯¼transformationï¼Œç½‘ç»œçš„ç»“æ„encoderä¾ç„¶æ˜¯ä¸€ä¸ªBi-LSTM
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190254.png)
      * æœ‰ä¸¤ç§Actorå½¢å¼ï¼ŒNet2Wideræˆ–è€…Net2Deeper
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190452.png)
      * Widerç”¨ä¸€ä¸ªå…¬ç”¨çš„sigmoidåˆ†ç±»å™¨æ¥ç”Ÿæˆåé¦ˆä¿¡æ¯ï¼Œç¡®å®šæ˜¯å¦åœ¨è¯¥æ—¶åˆ»éœ€è¦expand
      * Net2Deeperåˆ™æ˜¯æ˜¯å¦éœ€è¦insertä¸€ä¸ªæ–°çš„layer
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200216190710.png)
      * Deeperç”¨ä¸€ä¸ªRNNæ¥å»ºæ¨¡æ˜¯å¦éœ€è¦åœ¨æŸä¸ªä½ç½®æ’å…¥ä¸€ä¸ªæ–°å±‚
      * å¯ä»¥æ’å…¥æ–°å±‚çš„ä½ç½®æ˜¯é¢„å…ˆå›ºå®šçš„ï¼Œä¾æ®poolingå±‚çš„ä½ç½®æŠŠç½‘ç»œåˆ†æˆå‡ ä¸ªblock
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:   
    * (ä¹Ÿå¯ä»¥è®¤ä¸ºæ˜¯muation-basedï¼Œä¸è¿‡æŒ‡å¯¼mutationäº§ç”Ÿçš„ä¸æ˜¯é—ä¼ æˆ–è€…SAè¿™ç±»heuristicçš„æ–¹æ³•ï¼Œä½†æ˜¯æ˜¯RL-Agent)


#### 4. [Learning Transferable Architectures for Scalable Image Recognition](https://arxiv.org/pdf/1707.07012.pdf)
* ğŸ”‘ Key:      
  * **Cell-based Search Space** (NASNet SS)   
* ğŸ“ Source:     
  * CVPR2018 / Google Brain 
* ğŸŒ± Motivation:  
  * å°†SearchSpaceåˆ†æˆå¤šä¸ªç›¸åŒCellçš„å †å 
* ğŸ’Š Methodology:
  * Convolutional Blocks Repeated many times
    	* ä¸€èˆ¬åˆ†æˆNä¸ªNormal Cellhåé¢æ¥ä¸€ä¸ªReduction Cell
  * åŸºäºRNNçš„Controller
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185419.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185525.png)
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308185626.png) 
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:    
	* Regularization Technique - Schedule Drop Path



#### 5. [SMASH: One-Shot Model Architecture Search through HyperNetworks](https://arxiv.org/abs/1708.05344)

* ğŸ”‘ Key:  
  * **HyperNet** -> Produce Weight
  * **One-shot NAS Framework** -> Shared Weights      
* ğŸ“ Source:  
  * ICLR2017 
* ğŸŒ± Motivation: 
  * **one-shotè¡¨ç¤ºä»ä¸€ä¸ªHyperNetä¸­å–å‡ºæ¶æ„ï¼Œä¸éœ€è¦trainingä½œevaluationï¼Œé‡‡ç”¨weight sharing** 
  * HyperNetç”¨æ¥å¯¹ç»™å®šçš„æ¶æ„ç”ŸæˆWeightsï¼Œåšåˆ°Fast Eavluation
* ğŸ’Š Methodology: 
   * HyperNet - Training an auxiliary HyperNet to generate weights
     * åŠ é€Ÿarch selection
     * ä»binary codedåˆ°optimal architecture weightsçš„mappingï¼Œåªè®­ç»ƒoutput layer(?)
     * å…¶è®­ç»ƒæ˜¯ç”¨gradient-basedçš„æ–¹å¼æ¥åšåˆ°çš„
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:      
   * é€‰å–archæ¥evluateçš„æ–¹å¼
     * Memory-bank viewï¼Œå°†å…¶ä½œä¸ºbinary vector
     * æ˜¯å¦æ„å‘³ç€ä¼šéå†æ•´ä¸ªsearch spaceï¼Ÿ
   * æ–‡ç« ä¸­æåˆ°äº†è¯´*è®­ç»ƒä¸€ä¸ªarchå¼€å§‹çš„éƒ¨åˆ†æ˜¯æ•´ä½“accçš„ä¸€ä¸ªinsight*
     * ä¸€èˆ¬çš„æ–¹æ³•æŠŠarchçš„perfçœ‹ä½œä¸€ä¸ªblack boxï¼Œç”¨BOæˆ–è€…rså»æœç´¢ï¼Œä¹Ÿæœ‰early-stoppingè¿™æ ·çš„ç­–ç•¥ 
   * èƒ½å¤ŸæŠ›å¼ƒå…¶ä»–æ‰€æœ‰çš„hyper-paramä»¥åŠdynamic-regulariaztionçš„ä¸œè¥¿
     * æ¯”å¦‚lr schedule
     * æ¯”å¦‚DropPathä¹‹ç±»çš„ä¸œè¥¿
   * [Meta-Pruning(ICCV2019)](https://arxiv.org/pdf/1903.10258)å’Œè¿™ä¸ªæœ‰ç‚¹åƒçš„


#### 6. [Hierarchical Representations for Efficient Architecture Search](https://shimo.im/sheets/TkdXd9ptKTjDY83R/MODOC)
* ğŸ”‘ Key:     
  * **Hierarchical Search Space** 
* ğŸ“ Source:   
  * ICLR2018 / Google Brain   
* ğŸŒ± Motivation:  
  * æµè¡Œçš„Cell-Basedçš„æ¶æ„ç›¸å¯¹generalï¼Œä½†æ˜¯æœ‰predefinedçš„meta-arch(æ¯”å¦‚è¿™å‡ ä¸ªCellåº”è¯¥æ€ä¹ˆå †å ä¹‹ç±»çš„)ä¸å¤Ÿgeneral
* ğŸ’Š Methodology: 
  * hierarchical genetic representation
    * æ¨¡ä»¿çš„æ˜¯modularized design pattern
  * é‡‡ç”¨äº†EAï¼ŒæŒ‡å‡ºæœ€naiveçš„random searchä¹Ÿå¯ä»¥è·å¾—ä¸é”™çš„æ•ˆæœ
  * flat representation - å°†NNä½œä¸ºä¸€ä¸ªDAG
    * å°çš„graph motifç»„æˆä¸€ä¸ªå¤§çš„graph motif
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308204233.png)
  * Tournament Selection Search
* ğŸ“ Exps:       
* ğŸ’¡ Ideas: 

#### 7. [PNAS - Progressive Neural Architecture Search](https://arxiv.org/abs/1712.00559)
* ğŸ”‘ Key:      
  * **Progressive** -> Easy2Hard
* ğŸ“ Source:
  * ECCV 2018 / Google AI      
* ğŸŒ± Motivation:  
  * Progressive (Simple 2 Complex)
* ğŸ’Š Methodology:
  * Cell-based Search Space 
  * NO RL or EA, Sequential-Model-Based Method
  * æ¯ä¸€æ­¥åšä¸€ä¸ªå±€éƒ¨çš„Heuristic Searchï¼Œä»¥å‰ä¸€æ­¥çš„Predictoræ¥é€‰å–ä¸‹ä¸€æ¬¡çš„Predictor
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:   

#### 8. [Efficient Neural Architecture Search via Parameter Sharing](https://arxiv.org/pdf/1802.03268.pdf)

* ğŸ”‘ Key:
  * **SuperNet**
  * **One-Shot** -> Shared Weightsï¼ˆCurrent Popular Flowï¼‰     
* ğŸ“ Source:      
  * ICML 2018 / Google Brain
* ğŸŒ± Motivation:
  * æ„å»ºä¸€ä¸ªSupernetï¼Œè®¤ä¸ºæ‰€æœ‰æ¶æ„éƒ½æ˜¯è¿™ä¸ªSuperNetçš„Computation Graphçš„ä¸€ä¸ªSubGraphï¼Œå„ä¸ªChild Module Share Params  
* ğŸ’Š Methodology:
   * Controlleråœ¨ä¸€ä¸ªåºå¤§çš„Compuation Graphä¸Šæœç´¢Subgraphä½œä¸ºå­å›¾
    * å¯¹äºchild modules Share Parameter  
  * RNN Controller
    * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200308232519.png)
      * é‡‡æ ·å‡ºé“¾æ¥å…³ç³»
      * ä»¥åŠnodeæ˜¯ä»€ä¹ˆ
    * è¿™ä¸ªcontrolleræ˜¯æ€ä¹ˆè®­ç»ƒçš„ï¼Ÿ
      * Policy Gradientï¼Ÿ
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  



#### 9. [DARTSï¼šDifferentiable Architecture Search](https://arxiv.org/pdf/1806.09055.pdf)

* ğŸ”‘ Key:     
  * **ç¦»æ•£æœç´¢ -> Gradient-based**    
* ğŸ“ Source:   
  * ICLR 2019 / Google Brain   
* ğŸŒ± Motivation: 
  * å°†åŸå…ˆçš„ç¦»æ•£æœç´¢ï¼Œæ”¹ä¸ºDifferentiableæ–¹å¼(Relax the Search space to be Differentiable),ä»¥æé«˜æ•ˆç‡ï¼
  * æ ¸å¿ƒåœ¨äºå¦‚ä½•ä¿®æ”¹Search Space 
* ğŸ’Š Methodology: 
  * Search Spaceå¦‚ä½•è®¾è®¡
    * ä¸€ä¸ªCellæ˜¯ä¸€ä¸ªDAGï¼ŒNodeæ˜¯ä¸€ä¸ªlatent representation(ä»£è¡¨Feature Map)ï¼Œæ¯ä¸ªæœ‰å‘çš„è¾¹å’ŒæŸç§æ“ä½œ(Op)æœ‰å…³
      * æ¯ä¸ªopä»¥softmaxæ¥relaxï¼Œå–å„ä¸ªå®é™…æ“ä½œ(Max-pool/Conv/No)å‘ç”Ÿçš„æ¦‚ç‡
    * è®¤ä¸ºæ¯ä¸ªcellæœ‰ä¸¤ä¸ªè¾“å…¥ä¸€ä¸ªè¾“å‡ºï¼Œå¯¹å·ç§¯å±‚è¾“å…¥æ¥è‡ªprevious 2 layer(? 2åº¦è¿‘é‚»å±…ï¼Ÿ)
    * N=7 Node,æ²¡æœ‰strided
  * Bi-level Optimization
    * Learn Arch and Weight at the same time
    * æ³¨æ„å®é™…å¯¹alphaå¯¼æ•°æ˜¯ç”¨çš„ï¼Œè€Œä¸æ˜¯å•çº¯çš„ \partial{L_val}/\partrial{\alpha}
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312222701.png)
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200312221224.png)
* ğŸ“ Exps:       
* ğŸ’¡ Ideas: 


#### 9-1. [SNAS-Stochastic NAS](https://arxiv.org/pdf/1812.09926.pdf)

* ğŸ”‘ Key: 
  * **Gumble Softmax** in DARTS    
* ğŸ“ Source:      
  * ICLR2019 / SenseTime
* ğŸŒ± Motivation:  
* ğŸ’Š Methodology: 
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  



#### 10. [NAO-Neural Architecture Optimization](https://arxiv.org/abs/1808.07233)

* ğŸ”‘ Key: 
  * Arch **Encoder-Decoder**   
* ğŸ“ Source:      
  * NIPS2018 / MSRA
* ğŸŒ± Motivation:  
  * Predictor-based
  * æ¶‰åŠäº†ä¸€ä¸ªArchçš„Encoderï¼Œå°†archæ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´ï¼Œåœ¨è¯¥ç©ºé—´ç”¨Gradientä¼˜åŒ–
* ğŸ’Š Methodology:
    * Discrete -> Continuos 
      * åŒ…å«äº†ä¸€ä¸ªencoderï¼Œå°†archæ˜ å°„åˆ°ä¸€ä¸ªè¿ç»­ç©ºé—´ï¼ŒåŒæ—¶è¿˜æ­é…ä¸€ä¸ªdecoder
      * è¿˜æœ‰predictor 
    * ä¸æ­¤ç±»ä¼¼çš„æ˜¯DARTSï¼Œè¯´DARTSè®¤ä¸ºæœ€å¥½çš„archæ˜¯å½“å‰weightä¸‹çš„argmaxï¼Œè€ŒNAOç›´æ¥ç”¨ä¸€ä¸ªdecoderæ˜ å°„å›æ¨¡å‹
      * è¿˜æœ‰ä¸€æ”¯æ˜¯Bayesian Optimizationï¼Œä½œè€…è®¤ä¸ºGPçš„æ€§èƒ½äºCovariance Functionçš„è®¾è®¡å¼ºç›¸å…³
    * Search Spaceè®¾è®¡
      * ä¸¤æ­¥ï¼Œé¦–å…ˆå†³å®š1ï¼‰which 2 previous nodes as inputs 2)ç¡®å®šè¦ç”¨ä»€ä¹ˆop
    * Encoderå’ŒDecoderéƒ½æ˜¯LSTMï¼Œpredictoræ˜¯ä¸€ä¸ªmean-poolingåŠ mlp
    * ä¸‰è€…Jointly Train
      * è®¤ä¸ºpredictor could work as regularizationå»é¿å…encoderåªå¯¹åº”decoderçš„ç»“æœï¼Œè€Œæ²¡æœ‰æ­£å¸¸è¡¨å¾
        * è¿™ä¸€æ­¥å’Œä¼ ç»ŸVAEä¸­çš„åŠ noiseä¸€è‡´
    * è®¤ä¸ºweight-sharingå’ŒNAOæ˜¯complementaryçš„
* ğŸ“ Exps:       
* ğŸ’¡ Ideas:  
    * symmetricçš„design:ä¸ºäº†ä¿è¯symmetricçš„æ¨¡å‹ï¼ˆå®é™…ä¸Šæ˜¯ä¸€ä¸ªæ¨¡å‹ï¼‰çš„embeddingä¸€è‡´ï¼Œpredictorç»™å‡ºå·®ä¸å¤šçš„ç»“æœ
      * ç”¨äº†Augmentationï¼ˆflipï¼‰æ¥è®­ç»ƒencoder









---


#### [Overcoming Multi-Model Forgetting in One-Shot NAS with DiversityMaximization](https://shiruipan.github.io/publication/cvpr-2020-zhang/)
* ğŸ”‘ Key:
  * è§£å†³ä¼ ç»Ÿçš„Shared-Weightsæ–¹æ³•åœ¨ä¼˜åŒ–æ–°çš„æ¶æ„çš„æ—¶å€™è€çš„æ¶æ„ç²¾åº¦ä¼šä¸‹é™(Catastrophic Forgetting)çš„é—®é¢˜
* ğŸ“ Sourceï¼š
  * CVPR 2020
* ğŸŒ± Motivation: 
    * ä¼ ç»Ÿçš„OneShotæ–¹å¼è®¤ä¸ºJointly Optimized Supernet Weightsæ˜¯æœ€ä¼˜çš„
    * ä½†æ˜¯sequentially train archs with partially-shared weightsä¼šå¯¼è‡´Catastrophic Forgetting
    * æ–‡ç« æ ¸å¿ƒæŠŠOne-ShotNASçœ‹æˆä¸€ä¸ªContinual Learningçš„é—®é¢˜(Constrained Optimzation,learning of current arch should not degrade previous much)
* ğŸ’Š Methodology:
    * NSAS(Search-based Architecture Selection) Loss Function 
    * Enforce the architectures inheriting weights from the supernet in current step perform better than last step
    * å¦‚æœç´¯è®¡çš„è¯è¦æ±‚ä¼šå¤ªé«˜äº†ï¼Œæ‰€ä»¥ä¸æ˜¯é™å®šå…¨éƒ¨çš„Previous Archï¼Œè€Œæ˜¯é€‰æ‹©å…¶ä¸­çš„ä¸€ä¸ªSubset(å¯¹äºå¦‚ä½•é€‰å®šè¿™ä¸ªSubsetæ˜¯å‡å®šSubsetä¸­çš„Archè¦æœ‰Diversity-æ‰¾åˆ°æœ€å¤§Diversityçš„è¿‡ç¨‹å°±æ˜¯æ‰€ä¸ºçš„Novelty Search)
    * å®ç°çº¦æŸçš„æ–¹å¼æ˜¯åŠ ä¸€ä¸ªSoft Regularization


#### [One-Shot Neural Architecture Search via Self-Evaluated Template Network](https://arxiv.org/abs/1910.05733)
* ğŸ”‘ Key:
  * ä¼ ç»Ÿçš„Evluationæ…¢ï¼ŒShared Weightsçš„æ–¹å¼é€‰å–å»Evaluateçš„ç»„ä»¶çš„æ—¶å€™æ˜¯Randomçš„ï¼Œä¸å¤ŸInstructive
* ğŸ“ Sourceï¼š
  * ICCV 2019 / Xuanyi Dong, Yi Yang
* ğŸŒ± Motivation: 
  * æå‡ºäº†ä¸€ä¸ªSETN(Self Evaluated Template Network)
    * ä¸€ä¸ªEvaluatorå»é¢„æµ‹æœ‰æ›´ä½Valid Lossçš„æ¶æ„(ç±»ä¼¼ä¸€ä¸ªPredictor)
    * ä¸€ä¸ªæ¨¡æ¿Templateç½‘ç»œå»Shared Params,åŒ…å«äº†æ‰€æœ‰çš„Candidate
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402210522.png)
    * çœ‹ä¸Šå»å°±æ˜¯å¯¹Shared-weightsåŠ äº†ä¸€ä¸ªPredictorä½œä¸ºControllerï¼Œå»ä»ä¸€ä¸ªæ‰€è°“çš„Templateç½‘ç»œä¸­é‡‡æ ·å‡ºå­æ¶æ„ï¼ŒControlleræ¥å†³å®šæ€ä¹ˆé‡‡(è€Œä¸æ˜¯éšæœºé‡‡æ ·)
  * Nä¸ªCell
    * æ¯ä¸ªCellä¸­Bä¸ªBlock
    * æ¯ä¸ªBlockå¯èƒ½æ˜¯4å…ƒç»„
  * Candidate Network 
    * Contain All candidate CNN in search space
    * train stochsticly - uniformly sample 1 candidate and only optimize its params
      * Optimize each with equal possibility
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402212418.png)
      * Iæ˜¯inputæ˜¯çº¯éšæœºSampleï¼ŒFæ˜¯Functionï¼Œå…¶ä¸­çš„orderæŒ‡çš„æ˜¯å†ä¸€ä¸ªé›†åˆOä¸­é‡‡æ ·ï¼Œå…¶ä¸­f1çš„indexä¸€å®šè¦å°äºf2
    * Evaluatorï¼š
      * Encode one CNN candidate as a set of quadruples
      * ä»categorical distribution sampleå‡ºä¸€ä¸ªchoiceï¼Œç”¨softmax normalized valueä½œä¸ºvectorå€¼
      * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200402213049.png)
* ğŸ’¡ Ideas:
    * æŒ‡å‡ºæœ‰äº›Shared-Weightsçš„æ–¹æ³•ä¼šshared parameter with a learnable distribution of archs
      * ä½œè€…è®¤ä¸ºè¿™æ ·ä¼šæœ‰biasï¼Œå› ä¸ºç›¸å¯¹lightweightçš„modelä¼šæ›´å¿«æ”¶æ•›ï¼Œlearnable distribution will bias to these model
      * â€œthe Matthew effectâ€ to refer that some quickly-converged candidates will get more chances  to be further optimized in some NAS algorithms


#### [GATES]()
* ğŸ”‘ Key: 
  * NASä¸­çš„**Predictor**é—®é¢˜ï¼Œæä¾›ä¸€ä¸ªæ›´å¥½çš„Encoder
* ğŸ“ Source
  * Arxiv & THU EE
* ğŸŒ± Motivation: 
  * Current Encoder model topological information implicitly
  * åŸæœ¬çš„gcnä¹‹ç±»çš„encoderæ–¹å¼edges stand for notion of affinity, feature on node
  * View NN as Data-Processing Graph
* ğŸ’Š Methodology:
  * ![](https://github.com/A-suozhang/MyPicBed/raw/master/img/20200404200848.png)
  * è®¤ä¸ºä¼˜ç‚¹åœ¨äºèƒ½å¤Ÿç›´æ¥Handle Topological isomorphism,ä»¥åŠå¯¹Information Processingçš„å»ºæ¨¡
* ğŸ“ Exps: 
  * Nb101/201ä¸ŠKDï¼ŒNatKï¼ŒPatK
* ğŸ’¡ Ideas:
  * ç›¸å…³æ–‡ç« NAOï¼Œç”¨ä¸€ä¸ªEncoder-Decoderæ¥åš
  * In the Kendall's Tau measure, all discordant pairs are treated equally
  * å…¶ä»–æŒ‡æ ‡(å› ä¸ºkendall tauå…¶å®è€ƒè™‘äº†å¾ˆå¤špoor archçš„ç›¸å¯¹å…³ç³»ï¼Œå¯¹äºNASæ¥è¯´ç”¨å¤„ä¸å¤§)
    * NatKï¼š predictç»™å‡ºçš„Kä¸ªä¸­æœ€å¥½çš„å®é™…rank
    * PatKï¼š Predictor TopKåœ¨Gt TopKä¸­çš„æ¯”ä¾‹
  * Nb101 - 432k archs - Op on Node
  * Nb201 - 15625 - OP on Edge














## Reference

* [Awesome-NAS](https://github.com/D-X-Y/Awesome-NAS)

 
